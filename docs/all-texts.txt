Kotlin DataFrame is a JVM Kotlin library for in-memory data manipulation.     First, pick up the latest version of DataFrame  .   If you wish to play with data in interactive mode, setup  and run DataFrame there  If you have some JVM project, just add a dependency on DataFrame like it's described on  site    Hope that this documentation will help you to implement all the things you want to do with your data. To get inspiration, take a look at  folder.  will quickly familiarize you with the power of DataFrame, and other notebooks and projects will show you some applications of DataFrame in practical data analysis.   If you found a bug, or have an idea for a new feature, please  in DataFrame GitHub repository.  If you wish to contribute, you're welcome! Choose an issue you like, let us know that you're working on it, and prepare a  . We'll review and merge it if everything goes well.  You can also give us feedback or ask a question in  channel of a Kotlin Slack.  Good luck!
This documentation is written in such a way that it could be read sequentially and in this case, it provides all necessary information about the library, but at the same time the  section could be used as an API reference    Data frame is an abstraction for working with structured data. Essentially it’s a 2-dimensional table with labeled columns of potentially different types. You can think of it like a spreadsheet or SQL table, or a dictionary of series objects.  The handiness of this abstraction is not in the table itself but in a set of operations defined on it. Kotlin Dataframe library is an idiomatic Kotlin DSL defining such operations. The process of working with data frame is often called data wrangling which is the process of transforming and mapping data from one "raw" data form into another format that is more appropriate for analytics and visualization. The goal of data wrangling is to assure quality and useful data.      — Kotlin Dataframe is able to read and present data from different sources including not only plain CSV but also JSON . That’s why it has been designed hierarchical and allows nesting of columns and cells.     — hierarchical data layout also opens a possibility of converting any objects structure in application memory to a data frame and vice versa.    Safe — Kotlin Dataframe provides a mechanism of on-the-fly  that correspond to the columns of frame. In interactive notebooks like Jupyter or Datalore, the generation runs after each cell execution. In IntelliJ IDEA there's a Gradle plugin for generation properties based on CSV and Json. Also, we’re working on a compiler plugin that infers and transforms data frame schema while typing.  The generated properties ensures you’ll never misspell column name and don’t mess up with its type, and of course nullability is also preserved.    Generic — columns can store objects of any type, not only numbers or strings.     — if all columns of dataframe are presented in some other dataframe, then the first one could be a superclass for latter. Thus, one can define a function on an interface with some set of columns and then execute it in a safe way on any dataframe which contains this set of columns.    Immutable — all operations on DataFrame produce new instance, while underlying data is reused wherever it's possible     Basics:  val df = DataFrame.read("titanic.csv")  // filter rows df.filter { survived && home.endsWith("NY") && age in 10..20 } // add column df.add("birthYear") { 1912 - age } // sort rows df.sortByDesc { age } // aggregate data df.groupBy { pclass }.aggregate { maxBy { age }.name into "oldest person" count { survived } into "survived" }  Create:  // create columns val fromTo by columnOf("LoNDon_paris", "MAdrid_miLAN", "londON_StockhOlm", "Budapest_PaRis", "Brussels_londOn") val flightNumber by columnOf(10045.0, Double.NaN, 10065.0, Double.NaN, 10085.0) val recentDelays by columnOf("23,47", null, "24, 43, 87", "13", "67, 32") val airline by columnOf("KLM(!)", "{Air France} (12)", "(British Airways. )", "12. Air France", "'Swiss Air'") // create dataframe val df = dataFrameOf(fromTo, flightNumber, recentDelays, airline)  Clean:  // typed accessors for columns // that will appear during // dataframe transformation val origin by column<String>() val destination by column<String>() val dfClean = df // fill missing flight numbers .fillNA { flightNumber }.with { prev()!!.flightNumber + 10 } // convert flight numbers to int .convert { flightNumber }.toInt() // clean 'Airline' column .update { airline }.with { "([a-zA-Z\\s]+)".toRegex().find(it)?.value ?: "" } // split 'From_To' column into 'From' and 'To' .split { fromTo }.by("_").into(origin, destination) // clean 'From' and 'To' columns .update { origin and destination }.with { it.lowercase().replaceFirstChar(Char::uppercase) } // split lists of delays in 'RecentDelays' into separate columns // 'delay1', 'delay2'... and nest them inside original column `RecentDelays` .split { recentDelays }.inward { "delay$it" } // convert string values in `delay1`, `delay2` into ints .parse { recentDelays }  Aggregate:  // group by flight origin dfClean.groupBy { From into "origin" }.aggregate { // we are in the context of single data group // number of flights from origin count() into "count" // list of flight numbers flightNumber into "flight numbers" // counts of flights per airline airline.valueCounts() into "airlines" // max delay across all delays in `delay1` and `delay2` recentDelays.maxOrNull { delay1 and delay2 } into "major delay" // separate lists of recent delays for `delay1`, `delay2` and `delay3` recentDelays.implode(dropNulls = true) into "recent delays" // total delay per city of destination pivot { To }.sum { recentDelays.intCols() } into "total delays to" }
By nature data frames are dynamic objects, column labels depend on input source and also new columns could be added or deleted while wrangling. Kotlin in contrast is a statically typed language and all types are defined and verified ahead of execution. That's why creating flexible, handy and at the same time, safe API to a data frame is a tricky thing.  In Kotlin Dataframe we provide four different ways to access data, and while they are essentially different, they look pretty similar in data wrangling DSL.   Here's a list of all API's in the order of increasing their safeness.       Columns accessed by string representing their name. Type-checking is on runtime, name-checking is also on runtime.       Every column has a descriptor, a variable that representing its name and type.       Columns accessed by  of some class. The name and type of column should match the name and type of property     Extension access properties are generating based on dataframe schema. Name and type of properties infers from name and type of corresponding columns.     Here's an example of how the same operations can be performed via different access APIs  In the most of the code snippets in this documentation there's a tab selector that allows switching across access APIs      val df = DataFrame.read("titanic.csv")      df.add("lastName") { name.split(",").last() } .dropNulls { age } .filter { survived && home.endsWith("NY") && age in 10..20 }        DataFrame.read("titanic.csv") .add("lastName") { "name"<String>().split(",").last() } .dropNulls("age") .filter { "survived"<Boolean>() && "home"<String>().endsWith("NY") && "age"<Int>() in 10..20 }        val survived by column<Boolean>() val home by column<String>() val age by column<Int?>() val name by column<String>() val lastName by column<String>() DataFrame.read("titanic.csv") .add(lastName) { name().split(",").last() } .dropNulls { age } .filter { survived() && home().endsWith("NY") && age()!! in 10..20 }        data class Passenger( val survived: Boolean, val home: String, val age: Int, val lastName: String ) val passengers = DataFrame.read("titanic.csv") .add(Passenger::lastName) { "name"<String>().split(",").last() } .dropNulls(Passenger::age) .filter { it[Passenger::survived] && it[Passenger::home].endsWith("NY") && it[Passenger::age] in 10..20 } .toListOf<Passenger>()        is the simplest one and the most unsafe of all. The main advantage of it is that it can be used at any time, including accessing new columns in chain calls. So we can write something like:  df.add("weight") { ... } // add a new column `weight`, calculated by some expression .sortBy("weight") // sorting dataframe rows by its value  So we don't need to interrupt a method chain and declare a column accessor or generate new properties.  In contrast, generated  are the most convenient and safe API. Using it you can be always sure that you work with correct data and types. But its bottleneck — the moment of generation. To get new extension properties you have to run a cell in a notebook, which could lead to unnecessary variable declarations. Currently, we are working on compiler a plugin that generates these properties on the fly while user typing.   is a kind of trade-off between safeness and ahead of the execution type declaration. It was designed to write code in IDE without notebook experience. It provides type-safe access to columns but doesn't ensure that the columns really exist in a particular dataframe.   is useful when you have already declared classed in application business logic with fields that correspond columns of dataframe.    API  Type-checking  Column names checking  Column existence checking    Strings  Runtime  Runtime  Runtime    Column Accessors  Compile-time  Compile-time  Runtime    `KProperty` Accessors  Compile-time  Compile-time  Runtime    Extension Properties Accessors  Generation-time  Generation-time  Generation-time
String column names are the easiest way to access data in DataFrame:    DataFrame.read("titanic.csv") .add("lastName") { "name"<String>().split(",").last() } .dropNulls("age") .filter { "survived"<Boolean>() && "home"<String>().endsWith("NY") && "age"<Int>() in 10..20 }    Note that if data frame doesn’t contain column with the string provided, or you try to cast to the wrong type it will lead to runtime exception.
For frequently accessed columns type casting can be reduced by  :    val survived by column<Boolean>() // accessor for Boolean column with name 'survived' val home by column<String>() val age by column<Int?>() val name by column<String>() val lastName by column<String>()    Now columns can be accessed in a type-safe way using invoke operator:    DataFrame.read("titanic.csv") .add(lastName) { name().split(",").last() } .dropNulls { age } .filter { survived() && home().endsWith("NY") && age()!! in 10..20 }    Note that it still doesn’t solve the problem of whether the column actually exists in a data frame, but reduces type casting.
Kotlin Dataframe can be used as an intermediate structure for data transformation between two data formats. If either source or destination is a Kotlin object, e.g. data class, it is convenient to use its properties for typed data access in DataFrame . This can be done using :: expression that provides     data class Passenger( val survived: Boolean, val home: String, val age: Int, val lastName: String ) val passengers = DataFrame.read("titanic.csv") .add(Passenger::lastName) { "name"<String>().split(",").last() } .dropNulls(Passenger::age) .filter { it[Passenger::survived] && it[Passenger::home].endsWith("NY") && it[Passenger::age] in 10..20 } .toListOf<Passenger>()    By default, DataFrame uses name and returnType of KProperty for typed access to data. When column name differs from property name, use ColumnName annotation:    data class Passenger( @ColumnName("survived") val isAlive: Boolean, @ColumnName("home") val city: String, val name: String ) val passengers = DataFrame.read("titanic.csv") .filter { it.get(Passenger::city).endsWith("NY") } .toListOf<Passenger>()
When DataFrame is used within Jupyter Notebooks or Datalore with Kotlin Kernel, after every cell execution all new global variables of type DataFrame are analyzed and replaced with typed DataFrame wrapper with auto-generated extension properties for data access:    val df = DataFrame.read("titanic.csv")    Now data can be accessed by . member accessor    df.add("lastName") { name.split(",").last() } .dropNulls { age } .filter { survived && home.endsWith("NY") && age in 10..20 }    Extension properties are generated for DataSchema that is extracted from DataFrame instance after REPL line execution. After that DataFrame variable is typed with its own DataSchema , so only valid extension properties corresponding to actual columns in DataFrame will be allowed by the compiler and suggested by completion.  Also, extension properties  using  .  In notebooks generated properties won't appear and be updated until the cell has been executed. It often means that you have to introduce new variable frequently to sync extension properties with actual schema
DataFrame can represent hierarchical data structures using two special types of columns:    is a group of    is a column of    You can read DataFrame   or  preserving original tree structure.  Hierarchical columns can also appear as a result of some  :    produces ColumnGroup   produces FrameColumn   may produce FrameColumn   of FrameColumn will produce several ColumnGroups   converts ColumnGroup into FrameColumn   converts FrameColumn into ColumnGroup   converts ColumnGroups into FrameColumn  etc.
Kotlin Dataframe provides typed data access via  for type DataFrame<T> , where T is a marker class that represents DataSchema of DataFrame .  Schema of DataFrame is a mapping from column names to column types of DataFrame . It ignores order of columns in DataFrame , but tracks column hierarchy.  In Jupyter environment compile-time DataFrame schema is synchronized with real-time data after every cell execution.  In IDEA projects you can use  to extract schema from dataset and generate extension properties.   After execution of cell    val df = dataFrameOf("name", "age")( "Alice", 15, "Bob", null )    the following actions take place:   Columns in df are analyzed to extract data schema  Empty interface with DataSchema annotation is generated:   @DataSchema interface DataFrameType   Extension properties for this DataSchema are generated:   val ColumnsContainer<DataFrameType>.age: DataColumn<Int?> @JvmName("DataFrameType_age") get() = this["age"] as DataColumn<Int?> val DataRow<DataFrameType>.age: Int? @JvmName("DataFrameType_age") get() = this["age"] as Int? val ColumnsContainer<DataFrameType>.name: DataColumn<String> @JvmName("DataFrameType_name") get() = this["name"] as DataColumn<String> val DataRow<DataFrameType>.name: String @JvmName("DataFrameType_name") get() = this["name"] as String  Every column produces two extension properties:   Property for ColumnsContainer<DataFrameType> returns column  Property for DataRow<DataFrameType> returns cell value    df variable is typed by schema interface:   val temp = df  val df = temp.cast<DataFrameType>()   _Note, that object instance after casting remains the same. See  .   To log all these additional code executions, use cell magic  %trackExecution -all   In order to reduce amount of generated code, previously generated DataSchema interfaces are reused and only new properties are introduced  Let's filter out all null values from age column and add one more column of type Boolean :  val filtered = df.filter { age != null }.add("isAdult") { age!! > 18 }  New schema interface for filtered variable will be derived from previously generated DataFrameType :  @DataSchema interface DataFrameType1: DataFrameType  Extension properties for data access are generated only for new and overriden members of DataFrameType1 interface:  val ColumnsContainer<DataFrameType1>.age: DataColumn<Int> get() = this["age"] as DataColumn<Int> val DataRow<DataFrameType1>.age: Int get() = this["age"] as Int val ColumnsContainer<DataFrameType1>.isAdult: DataColumn<Boolean> get() = this["isAdult"] as DataColumn<Boolean> val DataRow<DataFrameType1>.isAdult: String get() = this["isAdult"] as Boolean  Then variable filtered is cast to new interface:  val temp = filtered  val filtered = temp.cast<DataFrameType1>()   You can define your own DataSchema interfaces and use them in functions and classes to represent DataFrame with specific set of columns:  @DataSchema interface Person { val name: String val age: Int }  After execution of this cell in Jupyter or annotation processing in IDEA, extension properties for data access will be generated. Now we can use these properties to create functions for typed DataFrame :  fun DataFrame<Person>.splitName() = split { name }.by(",").into("firstName", "lastName") fun DataFrame<Person>.adults() = filter { age > 18 }  In Jupyter these functions will work automatically for any DataFrame that matches Person schema:    val df = dataFrameOf("name", "age", "weight")( "Merton, Alice", 15, 60.0, "Marley, Bob", 20, 73.5 )    Schema of df is compatible with Person , so auto-generated schema interface will inherit from it:  @DataSchema(isOpen = false) interface DataFrameType : Person val ColumnsContainer<DataFrameType>.weight: DataColumn<Double> get() = this["weight"] as DataColumn<Double> val DataRow<DataFrameType>.weight: Double get() = this["weight"] as Double  Despite df has additional column weight , previously defined functions for DataFrame<Person> will work for it:    df.splitName()    firstName lastName age weight Merton Alice 15 60.000 Marley Bob 20 73.125    df.adults()    name age weight Marley, Bob 20 73.5  In JVM project you will have to   DataFrame explicitly to the target interface:  df.cast<Person>().splitName()   Sometimes it is convenient to extract reusable code from Jupyter notebook into Kotlin JVM library. If this code uses  , schema interfaces should also be extracted. In order to enable support them in Jupyter, you should register them in library  with useSchema function:  @DataSchema interface Person { val name: String val age: Int } fun DataFrame<Person>.countAdults() = count { it[Person::age] > 18 } @JupyterLibrary internal class Integration : JupyterIntegration() { override fun Builder.onLoaded() { onLoaded { useSchema<Person>() } } }  After loading this library into Jupyter notebook, schema interfaces for all DataFrame variables that match Person schema will derive from Person    val df = dataFrameOf("name", "age")( "Alice", 15, "Bob", 20 )    Now df is assignable to DataFrame<Person> and countAdults is available:  df.countAdults()
In Gradle project Kotlin DataFrame provides   Annotation processing for generation of extension properties  Annotation processing for DataSchema inference from datasets.  Gradle task for DataSchema inference from datasets.    To use  in Gradle project you should  .   Declare data schemas in your code and use them to access data in DataFrames. A data schema is a class or interface annotated with @DataSchema :  import org.jetbrains.kotlinx.dataframe.annotations.DataSchema @DataSchema interface Person { val name: String val age: Int }     val df = dataFrameOf("name", "age")( "Alice", 15, "Bob", 20 ).cast<Person>() // age only available after executing `build` or `kspKotlin`! val teens = df.filter { age in 10..19 } teens.print()     Specify schema with preferred method and execute the build task.    ImportDataSchema annotation must be above package directive. You can put this annotation in the same file as data processing code. You can import schema from URL or relative path of the file. Relative path by default is resolved to project root directory. You can configure it by   dataframe.resolutionDir option to preprocessor  Note that due to incremental processing, imported schema will be re-generated only if some source code has changed from previous invocation, at least one character  For the following configuration, file Repository.Generated.kt will be generated to build/generated/ksp/ folder in the same package as file containing the annotation.  @file:ImportDataSchema( "Repository", "https://raw.githubusercontent.com/Kotlin/dataframe/master/data/jetbrains_repositories.csv", ) import org.jetbrains.kotlinx.dataframe.annotations.ImportDataSchema  See KDocs for ImportDataSchema in IDE or  for more details    Put this in build.gradle or build.gradle.kts For the following configuration, file Repository.Generated.kt will be generated to build/generated/dataframe/org/example folder.  dataframes { schema { data = "https://raw.githubusercontent.com/Kotlin/dataframe/master/data/jetbrains_repositories.csv" name = "org.example.Repository" } }  See  and  for more details.    After build , the following code should compile and run:    // Repository.readCSV() has argument 'path' with default value https://raw.githubusercontent.com/Kotlin/dataframe/master/data/jetbrains_repositories.csv val df = Repository.readCSV() // Use generated properties to access data in rows df.maxBy { stargazersCount }.print() // Or to access columns in dataframe. print(df.fullName.count { it.contains("kotlin") })
You can use DataFrame in different environments — as any other JVM library. The following sections will show how to use DataFrame in  ,  and in a  .   You can use DataFrame in Jupyter Notebook and in Jupyter Lab. To start, install the latest version of  and start your favorite Jupyter client from the command line, for example:  jupyter notebook  In the notebook you only have to write single line to start using dataframe:  %use dataframe  In this case the version which is bundled with the kernel, will be used. If you want to always use the latest version, add another magic before %use dataframe :  %useLatestDescriptors %use dataframe  If you want to use specific version of DataFrame, you can specify it in brackets:  %use dataframe(0.8.0)  After loading, all essential types will be already imported, so you can start using DataFrame. Enjoy!   To start with DataFrame in Datalore, create a Kotlin notebook first:    As the Notebook you've created is actually a Jupyter notebook, you can follow the instructions in the  to turn DataFrame on. The simplest way of doing this is shown on screenshot:     DataFrame is published to Maven Central, so you can simply add the following line to your Kotlin DSL buildscript to depend on it:     dependencies { implementation("org.jetbrains.kotlinx:dataframe:<version>") }    dependencies { implementation 'org.jetbrains.kotlinx:dataframe:<version>' }     If you want to avoid adding unnecessary dependency, you can choose whatever you need:    dependencies { // Artifact containing all APIs and implementations implementation("org.jetbrains.kotlinx:dataframe-core:<version>") // Optional formats support implementation("org.jetbrains.kotlinx:dataframe-excel:<version>") implementation("org.jetbrains.kotlinx:dataframe-arrow:<version>") }    dependencies { // Artifact containing all APIs and implementations implementation 'org.jetbrains.kotlinx:dataframe-core:<version>' // Optional formats support implementation 'org.jetbrains.kotlinx:dataframe-excel:<version>' implementation 'org.jetbrains.kotlinx:dataframe-arrow:<version>' }     We provide a Gradle plugin that generates interfaces by your data. To use it in your project, pick up the latest version from  and follow the configuration:    plugins { kotlin("plugin.dataframe") version "<version>" } dependencies { implementation("org.jetbrains.kotlinx:dataframe:<version>") } // Make IDE aware of the generated code: kotlin.sourceSets.getByName("main").kotlin.srcDir("build/generated/ksp/main/kotlin/") // (Only if you use kotlint) Excludes for `kotlint`: tasks.withType<org.jmailen.gradle.kotlinter.tasks.LintTask> { exclude { it.name.endsWith(".Generated.kt") } exclude { it.name.endsWith("\$Extensions.kt") } }    plugins { id("org.jetbrains.kotlin.plugin.dataframe") version "<version>" } dependencies { implementation("org.jetbrains.kotlinx:dataframe:<version>") } // Make IDE aware of the generated code: kotlin.sourceSets.getByName("main").kotlin.srcDir("build/generated/ksp/main/kotlin/") // (Only if you use kotlint) Excludes for `kotlint`: tasks.withType(org.jmailen.gradle.kotlinter.tasks.LintTask).all { exclude { it.name.endsWith(".Generated.kt") } exclude { it.name.endsWith("\$Extensions.kt") } }    plugins { kotlin("plugin.dataframe") version "<version>" } kotlin { jvm() sourceSets { val jvmMain by getting { // Make IDE aware of the generated code: kotlin.srcDir("build/generated/ksp/jvmMain/kotlin/") dependencies { implementation("org.jetbrains.kotlinx:dataframe:<version>") } } } } // (Only if you use kotlint) Excludes for `kotlint`: tasks.withType<org.jmailen.gradle.kotlinter.tasks.LintTask> { exclude { it.name.endsWith(".Generated.kt") } exclude { it.name.endsWith("\$Extensions.kt") } }     If the code generated by the plugin isn't resolved in IDE, make sure you've configured source sets according to the snippet above. More information in    Note that it's better to use the same version for a library and plugin to avoid unpredictable errors. After plugin configuration you can try it out with  .   If you are using Maven, Ivy or Bazel to configure your build, you can still use DataFrame in your project. Just follow the instructions for your build system on  .
is a named, typed and ordered collection of elements   consists of one or several  with unique names and equal size   is a single row of  and provides a single value for every
DataFrame represents a list of  .  Columns in dataframe must have equal size and unique names.  Learn how to:
DataColumn represents a column of values. It can store objects of primitive or reference types, or other  .  See     name: String — name of the column, should be unique within containing dataframe  path: ColumnPath — path to the column, depends on the way column was retrieved from dataframe  type: KType — type of elements in the column  hasNulls: Boolean — flag indicating whether column contains null values  values: Iterable<T> — column data  size: Int — number of elements in the column    DataColumn instances can be one of three subtypes: ValueColumn , ColumnGroup or FrameColumn   Represents a sequence of values.  It can store values of primitive (integers, strings, decimals etc.) or reference types. Currently, it uses  as underlying data storage.   Container for nested columns. Is used to create column hierarchy.   Special case of  that stores other  as elements.  DataFrames stored in FrameColumn may have different schemas.  FrameColumn may appear after  from JSON or other hierarchical data structures, or after grouping operations such as  or  .   ColumnAccessors are used for  in DataFrame . ColumnAccessor stores column  (for top-level columns) or column path (for nested columns), has type argument that corresponds to  of thep column, but it doesn't contain any actual data.    val age by column<Int>() // Access fourth cell in the "age" column of dataframe `df`. // This expression returns `Int` because variable `age` has `ColumnAccessor<Int>` type. // If dataframe `df` has no column "age" or column "age" has type which is incompatible with `Int`, // runtime exception will be thrown. df[age][3] + 5 // Access first cell in the "age" column of dataframe `df`. df[0][age] * 2 // Returns new dataframe sorted by age column (ascending) df.sortBy(age) // Returns new dataframe with the column "year of birth" added df.add("year of birth") { 2021 - age } // Returns new dataframe containing only rows with age > 30 df.filter { age > 30 }     are created by   column . Column  should be passed as type argument, column  will be taken from the variable name.    val name by column<String>()    To assign column name explicitly, pass it as an argument.    val accessor by column<String>("complex column name")    You can also create column accessors for  and     val columns by columnGroup() val frames by frameColumn()    To reference nested columns inside  , invoke column<T>() on accessor to parent  :    val name by columnGroup() val firstName by name.column<String>()    You can also create virtual accessor that doesn't point to a real column but computes some expression on every data access:      val fullName by column(df) { name.firstName + " " + name.lastName } df[fullName]    val name by columnGroup() val firstName by name.column<String>() val lastName by name.column<String>() val fullName by column { firstName() + " " + lastName() } df[fullName]    val fullName by column { "name"["firstName"]<String>() + " " + "name"["lastName"]<String>() } df[fullName]     If expression depends only on one column, you can also use map :    val age by column<Int>() val year by age.map { 2021 - it } df.filter { year > 2000 }    To convert ColumnAccessor into  add values using withValues function:    val age by column<Int>() val ageCol1 = age.withValues(15, 20) val ageCol2 = age.withValues(1..10)
DataRow represents a single record, one piece of data within a     index(): Int — sequential row number in DataFrame , starts from 0  prev(): DataRow? — previous row ( null for the first row)  next(): DataRow? — next row ( null for the last row)  diff { rowExpression }: T — difference between results of  calculated for current and previous rows  values(): List<Any?> — list of all cell values from the current row  valuesOf<T>(): List<T> — list of values of given type  columnsCount(): Int — number of columns  columnNames(): List<String> — list of all column names  columnTypes(): List<KType> — list of all column types  namedValues(): List<NameValuePair<Any?>> — list of name-value pairs where name is a column name and value is cell value  namedValuesOf<T>(): List<NameValuePair<T>> — list of name-value pairs where value has given type  transpose(): DataFrame<NameValuePair<*>> — dataframe of two columns: name: String is column names and value: Any? is cell values  transposeTo<T>(): DataFrame<NameValuePair<T>> — dataframe of two columns: name: String is column names and value: T is cell values  getRow(Int): DataRow — row from DataFrame by row index  getRows(Iterable<Int>): DataFrame — dataframe with subset of rows selected by absolute row index.  relative(Iterable<Int>): DataFrame — dataframe with subset of rows selected by relative row index: relative(-1..1) will return previous, current and next row. Requested indices will be coerced to the valid range and invalid indices will be skipped  get(column): T — cell value by this row and given column  df() — DataFrame that current row belongs to    Row expressions provide a value for every row of DataFrame and are used in  ,  ,  ,  and other operations.    // Row expression computes values for a new column df.add("fullName") { name.firstName + " " + name.lastName } // Row expression computes updated values df.update { weight }.at(1, 3, 4).with { prev()?.weight } // Row expression computes cell content for values of pivoted column df.pivot { city }.with { name.lastName.uppercase() }    Row expression signature: DataRow.(DataRow) -> T . Row values can be accessed with or without it keyword. Implicit and explicit argument represent the same DataRow object.   Row condition is a special case of  that returns Boolean .    // Row condition is used to filter rows by index df.filter { index() % 5 == 0 } // Row condition is used to drop rows where `age` is the same as in previous row df.drop { diff { age } == 0 } // Row condition is used to filter rows for value update df.update { weight }.where { index() > 4 && city != "Paris" }.withValue(50)    Row condition signature: DataRow.(DataRow) -> Boolean   The following  are available for DataRow :   rowMax  rowMin  rowSum  rowMean  rowStd  rowMedian   These statistics will be applied only to values of appropriate types and incompatible values will be ignored. For example, if DataFrame has columns of type String and Int , rowSum() will successfully compute sum of Int values in a row and ignore String values.  To apply statistics only to values of particular type use -Of versions:   rowMaxOf<T>  rowMinOf<T>  rowSumOf<T>  rowMeanOf<T>  rowMedianOf<T>
Data transformation pipeline usually consists of several modification operations, such as filtering, sorting, grouping, pivoting, adding/removing columns etc. DataFrame API is designed in functional style so that the whole processing pipeline can be represented as a single statement with a sequential chain of operations. DataFrame object is immutable and all operations return a new DataFrame instance reusing underlying data structures as much as possible.    df.update { age }.where { city == "Paris" }.with { it - 5 } .filter { isHappy && age > 100 } .move { name.firstName and name.lastName }.after { isHappy } .merge { age and weight }.by { "Age: ${it[0]}, weight: ${it[1]}" }.into("info") .rename { isHappy }.into("isOK")     You can play with "people" dataset that is used in present guide     Simple operations (such as  or  ) return new DataFrame immediately, while more complex operations return an intermediate object that is used for further configuration of the operation. Let's call such operations multiplex .  Every multiplex operation configuration consists of:    that is used to select target columns for the operation  additional configuration functions  terminal function that returns modified DataFrame   Most multiplex operations end with into or with function. The following naming convention is used:   into defines column names for storing operation results. Used in  ,  ,  ,  ,  ,  ,  .  with defines row-wise data transformation with  . Used in  ,  ,  ,  .      — add columns   — add id column   — add rows   /  /  — get list of top-level columns, column names or column types   — number of top-level columns   — union rows from several dataframes   — change column values and/or column types   — pairwise correlation of columns   — number of rows that match condition   — number of unique rows   — cumulative sum of column values   — basic column statistics   /  — remove duplicated rows   /  /  /  /  — remove rows by condition   — duplicate rows   — spread lists and dataframes vertically into new rows   /  /  — replace missing values   /  — filter rows by condition   /  — find first row by condition   — remove column groupings recursively   /  — iterate over rows or columns   — conditional formatting for cell rendering   — convert pairs of column names and values into new columns   /  /  /  — get one or several columns   — group columns into    — group rows by key columns   — get first 5 rows of dataframe   — collapse column values into lists grouping by other columns   — infer column type from column values   — insert column   — join dataframes by key columns   /  — find last row by condition   — map columns into new  or    /  /  /  — max of values   /  /  — average of values   /  /  — median of values   — merge several columns into one   /  /  /  — min of values   — move columns or change column groupings   — try to convert strings into other types   /  /  — convert values into new columns   — remove columns   — rename columns   /  /  — reorder columns   — replace columns   — reverse rows   /  — get rows in direct or reversed order   — number of rows   — schema of columns: names, types and hierarchy   — select subset of columns   — reorder rows randomly   /  — get single row by condition   /  /  — sort rows   — split column values into new rows/columns or inplace into lists   /  /  — standard deviation of values   /  /  — sum of values   /  /  — get first/last rows   /  — export dataframe into a list of data classes   — export dataframe into a map from column names to column values   — remove column groupings   — update column values preserving column types   — Sequence of values traversed by row or by column   — counts for unique values   — slice dataframe by given key values    Some operations are shortcuts for more general operations:    ,  ,  are special cases of    is a special case of    ,  are special cases of    ,  ,  are special cases of    is a special case of    You can use these shortcuts to apply the most common DataFrame transformations easier, but you can always fall back to general operations if you need more customization.
There are several ways to create dataframes from the data that is already loaded into memory:    and then  into DataFrame  create and initialize DataFrame directly from values using vararg variants of the  .   into DataFrame   To learn how to read dataframes from files and URLs go to  .
This section describes ways to create  .   Returns new column with given elements. Column  is deduced from compile-time type of elements, column  is taken from the name of the variable.    // Create ValueColumn with name 'student' and two elements of type String val student by columnOf("Alice", "Bob")    To assign column name explicitly, use named infix function and replace by with = .    val column = columnOf("Alice", "Bob") named "student"    When column elements are columns themselves, it returns  :    val firstName by columnOf("Alice", "Bob") val lastName by columnOf("Cooper", "Marley") // Create ColumnGroup with two nested columns val fullName by columnOf(firstName, lastName)    When column elements are  it returns  :    val df1 = dataFrameOf("name", "age")("Alice", 20, "Bob", 25) val df2 = dataFrameOf("name", "temp")("Charlie", 36.6) // Create FrameColumn with two elements of type DataFrame val frames by columnOf(df1, df2)     Converts Iterable of values into column.    listOf("Alice", "Bob").toColumn("name")    To compute column type at runtime by scanning through actual values, set Infer.Type option.  To inspect values only for nullability set Infer.Nulls option.    val values: List<Any?> = listOf(1, 2.5) values.toColumn("data") // type: Any? values.toColumn("data", Infer.Type) // type: Number values.toColumn("data", Infer.Nulls) // type: Any     Converts Iterable of values into column of given type    val values: List<Any?> = listOf(1, 2.5) values.toColumnOf<Number?>("data") // type: Number?
This section describes ways to create  .   Returns  with given column names and values.    // DataFrame with 2 columns and 3 rows val df = dataFrameOf("name", "age")( "Alice", 15, "Bob", 20, "Charlie", 100 )      // DataFrame with 2 columns and 3 rows val df = dataFrameOf( "name" to listOf("Alice", "Bob", "Charlie"), "age" to listOf(15, 20, 100) )      val name by columnOf("Alice", "Bob", "Charlie") val age by columnOf(15, 20, 22) // DataFrame with 2 columns val df = dataFrameOf(name, age)      val names = listOf("name", "age") val values = listOf( "Alice", 15, "Bob", 20, "Charlie", 22 ) val df = dataFrameOf(names, values)      // Multiplication table dataFrameOf(1..10) { x -> (1..10).map { x * it } }      // 5 columns filled with 7 random double values: val names = (1..5).map { "column$it" } dataFrameOf(names).randomDouble(7) // 5 columns filled with 7 random double values between 0 and 1 (inclusive) dataFrameOf(names).randomDouble(7, 0.0..1.0).print() // 5 columns filled with 7 random int values between 0 and 100 (inclusive) dataFrameOf(names).randomInt(7, 0..100).print()      val names = listOf("first", "second", "third") // DataFrame with 3 columns, fill each column with 15 `true` values val df = dataFrameOf(names).fill(15, true)     DataFrame from Iterable<DataColumn> :    val name by columnOf("Alice", "Bob", "Charlie") val age by columnOf(15, 20, 22) listOf(name, age).toDataFrame()    DataFrame from Map<String, List<*>> :    val map = mapOf("name" to listOf("Alice", "Bob", "Charlie"), "age" to listOf(15, 20, 22)) // DataFrame with 2 columns map.toDataFrame()    DataFrame from Iterable of objects:    data class Person(val name: String, val age: Int) val persons = listOf(Person("Alice", 15), Person("Bob", 20), Person("Charlie", 22)) val df = persons.toDataFrame()    Scans object properties using reflection and creates  for every property. Scope of properties for scanning is defined at compile-time by formal types of objects in Iterable , so properties of implementation classes will not be scanned.  Specify depth parameter to perform deep object graph traversal and convert nested objects into  and  :    data class Name(val firstName: String, val lastName: String) data class Score(val subject: String, val value: Int) data class Student(val name: Name, val age: Int, val scores: List<Score>) val students = listOf( Student(Name("Alice", "Cooper"), 15, listOf(Score("math", 4), Score("biology", 3))), Student(Name("Bob", "Marley"), 20, listOf(Score("music", 5))) ) val df = students.toDataFrame(maxDepth = 1)    For detailed control over object graph transformation use configuration DSL. It allows you to exclude particular properties or classes from object graph traversal, compute additional columns and configure column grouping.    val df = students.toDataFrame { // add column "year of birth" from { 2021 - it.age } // scan all properties properties(maxDepth = 1) { exclude(Score::subject) // `subject` property will be skipped from object graph traversal preserve<Name>() // `Name` objects will be stored as-is without transformation into DataFrame } // add column group "summary" { "max score" from { it.scores.maxOf { it.value } } "min score" from { it.scores.minOf { it.value } } } }
When you work with data, you have to  it from disk or from remote URLs and  it on disk. This section describes how to do it. For now, CSV, TSV, JSON, XLS, XLSX, Apache Arrow formats are supported.
DataFrame supports CSV, TSV, JSON, XLS and XLSX, Apache Arrow input formats.  read method automatically detects input format based on file extension and content  DataFrame.read("input.csv")  Input string can be a file path or URL.   All these calls are valid:  import java.io.File import java.net.URL DataFrame.readCSV("input.csv") DataFrame.readCSV(File("input.csv")) DataFrame.readCSV(URL("https://raw.githubusercontent.com/Kotlin/dataframe/master/data/jetbrains_repositories.csv"))  All readCSV overloads support different options. For example, you can specify custom delimiter if it differs from , , charset and column names if your CSV is missing them    val df = DataFrame.readCSV( file, delimiter = '|', header = listOf("A", "B", "C", "D"), parserOptions = ParserOptions(nullStrings = setOf("not assigned")) )    Column types will be inferred from the actual CSV data. Suppose that CSV from the previous example had the following content:   A B C D  12 tuv 0.12 true  41 xyz 3.6 not assigned  89 abc 7.1 false   Dataframe schema we get is:  A: Int B: String C: Double D: Boolean?  DataFrame will try to parse columns as JSON, so when reading following table with JSON object in column D:   A D  12 {"B":2,"C":3}  41 {"B":3,"C":2}   We get this data schema where D is ColumnGroup with 2 children columns:  A: Int D: B: Int C: Int  For column where values are lists of JSON values:   A G  12 [{"B":1,"C":2,"D":3},{"B":1,"C":3,"D":2}]  41 [{"B":2,"C":1,"D":3}]   A: Int G: * B: Int C: Int D: Int   Basics for reading JSONs are the same: you can read from file or from remote URL.  DataFrame.readJson("https://covid.ourworldindata.org/data/owid-covid-data.json")  Note that after reading a JSON with a complex structure, you can get hierarchical dataframe: dataframe with ColumnGroup s and FrameColumn s.  Also note that type inferring process for JSON is much simpler than for CSV. JSON string literals are always supposed to have String type, number literals take different Number kinds, boolean literals are converted to Boolean .  Let's take a look at the following JSON:  [ { "A": "1", "B": 1, "C": 1.0, "D": true }, { "A": "2", "B": 2, "C": 1.1, "D": null }, { "A": "3", "B": 3, "C": 1, "D": false }, { "A": "4", "B": 4, "C": 1.3, "D": true } ]  We can read it from file    val df = DataFrame.readJson(file)    Corresponding dataframe schema will be  A: String B: Int C: Number D: Boolean?  Column A has String type because all values are string literals, no implicit conversion is performed. Column C has Number type because it's the least common type for Int and Double .   Add dependency:  implementation("org.jetbrains.kotlinx:dataframe-excel:$dataframe_version")  Right now DataFrame supports reading Excel spreadsheet formats: xls, xlsx.  You can read from file or URL.  Cells representing dates will be read as kotlinx.datetime.LocalDateTime . Cells with number values, including whole numbers such as "100", or calculated formulas will be read as Double  Sometimes cells can have wrong format in Excel file, for example you expect to read column of String:  IDS 100 <-- Intended to be String, but has wrong cell format in original .xlsx file A100 B100 C100  You will get column of Serializable instead (common parent for Double & String)  You can fix it using convert:    val df = dataFrameOf("IDS")(100.0, "A100", "B100", "C100") val df1 = df.convert("IDS").with(Infer.Type) { if (it is Double) { it.toLong().toString() } else { it } } df1["IDS"].type() shouldBe typeOf<String>()     Add dependency:  implementation("org.jetbrains.kotlinx:dataframe-arrow:$dataframe_version")  Dataframe supports reading from  and     val df = DataFrame.readArrowFeather(file)
DataFrame can be saved into CSV, TSV, JSON and XLS, XLSX formats.   You can write DataFrame in CSV format to file, to String or to Appendable (i.e. to Writer ).  Values of ColumnGroup, FrameColumn, i.e. AnyRow, AnyFrame will be serialized as JSON objects.    df.writeCSV(file)      val csvStr = df.toCsv(CSVFormat.DEFAULT.withDelimiter(';').withRecordSeparator(System.lineSeparator()))    ColumnGroup and FrameColumn values will be serialized as JSON strings.   You can write your dataframe in JSON format to file, to string or to Appendable (i.e. to Writer ).    df.writeJson(file)      val jsonStr = df.toJson(prettyPrint = true)     Add dependency:  implementation("org.jetbrains.kotlinx:dataframe-excel:$dataframe_version")  You can write your dataframe in XLS, XLSX format to a file, OutputStream or Workbook object.    df.writeExcel(file)    Values of ColumnGroup, FrameColumn, i.e. AnyRow, AnyFrame will be serialized as JSON objects.  If you work directly with Apache POI, you can use created Workbook and Sheets in your code:    /** * Do something with generated sheets. Here we set bold style for headers and italic style for first data column */ fun setStyles(sheet: Sheet) { val headerFont = sheet.workbook.createFont() headerFont.bold = true val headerStyle = sheet.workbook.createCellStyle() headerStyle.setFont(headerFont) val indexFont = sheet.workbook.createFont() indexFont.italic = true val indexStyle = sheet.workbook.createCellStyle() indexStyle.setFont(indexFont) sheet.forEachIndexed { index, row -> if (index == 0) { for (cell in row) { cell.cellStyle = headerStyle } } else { row.first().cellStyle = indexStyle } } } // Create a workbook (or use existing) val wb = WorkbookFactory.create(true) // Create different sheets from different data frames in the workbook val allPersonsSheet = df.writeExcel(wb, sheetName = "allPersons") val happyPersonsSheet = df.filter { person -> person.isHappy }.remove("isHappy").writeExcel(wb, sheetName = "happyPersons") val unhappyPersonsSheet = df.filter { person -> !person.isHappy }.remove("isHappy").writeExcel(wb, sheetName = "unhappyPersons") // Do anything you want by POI listOf(happyPersonsSheet, unhappyPersonsSheet).forEach { setStyles(it) } // Save the result file.outputStream().use { wb.write(it) } wb.close()
General information about DataFrame :    /  — number of rows   — number of distinct rows   — number of columns   — list of column names   — list of column types   — first n rows (default 5)   — schema of columns   — general statistics for every column
Returns number of top-level columns in DataFrame .
Returns number of rows in DataFrame .  rowsCount()  Same as
Returns number of distinct combinations of values in selected columns of DataFrame .      df.countDistinct { age and name }    val age by column<Int>() val name by columnGroup() df.countDistinct { age and name }    df.countDistinct("age", "name")     When columns are not specified, returns number of distinct rows in DataFrame .    df.countDistinct()
Returns list of names for top-level columns of DataFrame .
Returns list of types for top-level columns of DataFrame .
Returns DataFrame containing first n (default 5) rows.    df.head(3)    Similar to  .
Returns DataFrameSchema object with DataFrame schema description. It can be printed to see column structure.   are marked by indentation:    df.schema()    Output:  name: firstName: String lastName: String age: Int city: String? weight: Int? isHappy: Boolean   are marked with * :    df.groupBy { city }.schema()    Output:  city: String? group: * name: firstName: String lastName: String age: Int city: String? weight: Int? isHappy: Boolean
Returns DataFrame with general statistics for all  .  describe [ columns ]  ColumnGroups and FrameColumns are traversed recursively down to ValueColumns .  Collected statistics:   name — column name  path — path to the column (for hierarchical DataFrame )  type — type of values  count — number of rows  unique — number of unique values  nulls — number of null values  top — the most common not null value  freq — top value frequency  mean — mean value (for numeric columns)  std — standard deviation (for numeric columns)  min — minimal value (for comparable columns)  median — median value (for comparable columns)  max — maximum value (for comparable columns)     df.describe()    To describe only specific columns, pass them as an argument:      df.describe { age and name.all() }    val age by column<Int>() val name by columnGroup() df.describe { age and name.all() }    df.describe { "age" and "name".all() }
Get  or  :    df.columns() // List<DataColumn> df.rows() // Iterable<DataRow> df.values() // Sequence<Any?>    Learn how to:
df.age[1] df[1].age    val age by column<String>() df[age][1] df[1][age]    df["age"][1] df[1]["age"]
Iterate over rows:      for (row in df) { println(row.age) } df.forEach { println(it.age) } df.rows().forEach { println(it.age) }    val age by column<Int>() for (row in df) { println(row[age]) } df.forEach { println(it[age]) } df.rows().forEach { println(it[age]) }    for (row in df) { println(row["age"]) } df.forEach { println(it["age"]) } df.rows().forEach { println(it["age"]) }     Iterate over columns:    df.columns().forEach { println(it.name()) }    Iterate over cells:    // from top to bottom, then from left to right df.values().forEach { println(it) } // from left to right, then from top to bottom df.values(byRows = true).forEach { println(it) }
Get single  by  :    df[2]    Get single  by  :      df.single { age == 45 } df.first { weight != null } df.minBy { age } df.maxBy { name.firstName.length } df.maxByOrNull { weight }    val age by column<Int>() val weight by column<Int?>() val name by columnGroup() val firstName by name.column<String>() df.single { age() == 45 } df.first { weight() != null } df.minBy(age) df.maxBy { firstName().length } df.maxByOrNull { weight() }    df.single { "age"<Int>() == 45 } df.first { it["weight"] != null } df.minBy("weight") df.maxBy { "name"["firstName"]<String>().length } df.maxByOrNull("weight")
Return rows of DataFrame as Iterable<DataRow> .   Return rows of DataFrame in reversed order.
Returns the single  that matches the given  , or throws exception if there is no or more than one matching row.   Returns the single  that matches the given  , or null if there is no or more than one matching row.
Returns the first  that matches the given  , or throws exception if there is no matching rows.   Returns the first  that matches the given  , or null if there is no matching rows.
Returns the last  that matches the given  , or throws exception if there is no matching rows.   Returns the last  that matches the given  , or null if there is no matching rows.
Returns the first  that has the smallest value in the given column, or throws exception if DataFrame is empty.   Returns the first  that has the smallest value in the given column, or null if DataFrame is empty.
Returns the first  that has the largest value in the given column, or throws exception if DataFrame is empty.   Returns the first  that has the largest value in the given column, or null if DataFrame is empty.
Get single column by column name:      df.age df.name.lastName    val age by column<Int>() val name by columnGroup() val lastName by name.column<String>() df[age] df[lastName]    df["age"] df["name"]["firstName"]     Get single column by index (starting from 0):    df.getColumn(2) df.getColumnGroup(0).getColumn(1)
Return top-level columns of DataFrame as List<DataColumn<*>>
Return column by column name or  as  . Throws exception if requested column doesn't exist.      df.getColumn { age }    val age by column<Int>() df.getColumn { age }    df.getColumn("age")      Return top-level column by column name or  as  or null if requested column doesn't exist.      df.getColumnOrNull { age }    val age by column<Int>() df.getColumnOrNull(age)    df.getColumnOrNull("age")      Return top-level column by column name or  as  . Throws exception if requested column doesn't exist or is not a ColumnGroup .      df.getColumnGroup { name }    val name by columnGroup() df.getColumnGroup(name)    df.getColumnGroup("name")      Return list of selected columns.      df.getColumns { age and name }    val age by column<Int>() val name by columnGroup() df.getColumns { age and name }    df.getColumns("age", "name")
// TODO
Return Sequence of values from one or several columns of DataFrame .  values(byRows: Boolean = false) [ columns ]: Sequence<C>  Parameters:   columns (optional) — subset of columns for values extraction  byRows: Boolean = false — if true , data is traversed by rows, not by columns     df.values() df.values(byRows = true) df.values { age and weight }     In  and  aggregations values function yields list of column values for every aggregated data group.  df.groupBy { A }.values { B } df.pivot { A }.values { B } df.pivot { A }.groupBy { B }.values { C and D }
DataFrame object is immutable and all operations return a new instance of DataFrame .    DataFrame is a columnar data structure and is more oriented to column-wise operations. Most transformation operations start with  that selects target columns for the operation. Syntax of most column operations assumes that they are applied to columns, so they don't include word column in their naming.  On the other hand, Kotlin dataframe follows Koltin Collections naming for row-wise operations as DataFrame can be interpreted as a Collection of rows. The slight naming difference with Kotlin Collection is that all operations are named in imperative way: sortBy , shuffle etc.  Pairs of column/row operations:    columns /  rows   columns /  rows   columns /  rows   columns /  for rows   columns /  for rows   to unite columns /  to unite rows   Horizontal (column) operations:    — add columns   — add id column   — remove column groupings recursively   — group columns into    — insert column   — map columns into new  or    — merge several columns into one   — move columns or change column groupings   — remove columns   — rename columns   — reorder columns   — replace columns   — select subset of columns   — split values into new columns   — remove column grouping   Vertical (row) operations:    — add rows   — union rows from several dataframes   /  — remove duplicated rows   /  /  /  /  — remove rows by condition   — duplicate rows   — spread lists and dataframes vertically into new rows   /  — filter rows   — merge column values into lists grouping by other columns   — reverse rows   — reorder rows randomly   /  /  — sort rows   — split values into new rows   /  /  — get first/last rows   Value modification:    — convert values into new types   — try to convert String values into appropriate types   — update values preserving column types   /  /  — replace missing values   Reshaping:    /  /  — convert values into new columns   — convert pairs of column names and values into key and value columns   Learn how to:
Returns a DataFrame with rows at given indices:    df[0, 3, 4]    Returns a DataFrame with rows inside given index ranges (including boundary indices):    df[1..2] df[0..2, 4..5]     Returns a DataFrame containing first n rows    df.take(5)     Returns a DataFrame containing last n rows    df.takeLast(5)     Returns a DataFrame containing first rows that satisfy the given     df.takeWhile { isHappy }     Returns a DataFrame containing all rows except first n rows    df.drop(5)     Returns a DataFrame containing all rows except last n rows    df.dropLast() // default 1 df.dropLast(5)     Returns a DataFrame containing all rows except first rows that satisfy the given     df.dropWhile { !isHappy }
Return cross-section from the DataFrame .  Filters DataFrame by matching key values with key columns and removes key columns.  xs(vararg keyValues: Any?) [ { keyColumns } ]  When keyColumns are not specified, it takes first n columns in dfs order (looking inside ColumnGroups ), where n is a number of given keyValues .    df.xs("Charlie", "Chaplin") df.xs("Moscow", true) { city and isHappy }
Two ways to create DataFrame with a subset of columns:  indexing:      df[df.age, df.weight]    val age by column<Int>() val weight by column<Int?>() df[age, weight]    df["age", "weight"]     See   selecting:      df.select { age and weight }    val age by column<Int>() val weight by column<Int?>() df.select { age and weight } df.select(age, weight)    df.select { "age" and "weight" } df.select("age", "weight")     See
— keep only rows that satisfy to condition   — remove rows that satisfy to condition   — remove duplicate rows
Returns DataFrame with rows that satisfy       df.filter { age > 18 && name.firstName.startsWith("A") }    val age by column<Int>() val name by columnGroup() val firstName by name.column<String>() df.filter { age() > 18 && firstName().startsWith("A") } // or df.filter { it[age] > 18 && it[firstName].startsWith("A") }    df.filter { "age"<Int>() > 18 && "name"["firstName"]<String>().startsWith("A") }      Returns DataFrame with rows that have value true in given column of type Boolean .      df.filterBy { isHappy }    val isHappy by column<Boolean>() df.filterBy { isHappy }    df.filterBy("isHappy")
Removes all rows that satisfy       df.drop { weight == null || city == null }    val name by columnGroup() val weight by column<Int?>() val city by column<String?>() df.drop { weight() == null || city() == null } // or df.drop { it[weight] == null || it[city] == null }    df.drop { it["weight"] == null || it["city"] == null }      Remove rows with null values    df.dropNulls() // remove rows with null value in any column df.dropNulls(whereAllNull = true) // remove rows with null values in all columns df.dropNulls { city } // remove rows with null value in 'city' column df.dropNulls { city and weight } // remove rows with null value in 'city' OR 'weight' columns df.dropNulls(whereAllNull = true) { city and weight } // remove rows with null value in 'city' AND 'weight' columns     Remove rows with null , Double.NaN or Float.NaN values    df.dropNA() // remove rows containing null or Double.NaN in any column df.dropNA(whereAllNA = true) // remove rows with null or Double.NaN in all columns df.dropNA { weight } // remove rows where 'weight' is null or Double.NaN df.dropNA { age and weight } // remove rows where either 'age' or 'weight' is null or Double.NaN df.dropNA(whereAllNA = true) { age and weight } // remove rows where both 'age' and 'weight' are null or Double.NaN
Removes duplicate rows. The rows in the resulting DataFrame are in the same order as they were in the original DataFrame .    df.distinct()    If columns are specified, resulting DataFrame will have only given columns with distinct values.      df.distinct { age and name } // same as df.select { age and name }.distinct()    val age by column<Int>() val name by columnGroup() df.distinct { age and name } // same as df.select { age and name }.distinct()    df.distinct("age", "name") // same as df.select("age", "name").distinct()      Keep only the first row for every group of rows grouped by some condition.      df.distinctBy { age and name } // same as df.groupBy { age and name }.mapToRows { group.first() }    val age by column<Int>() val name by columnGroup() val firstName by name.column<String>() df.distinctBy { age and name } // same as df.groupBy { age and name }.mapToRows { group.first() }    df.distinctBy("age", "name") // same as df.groupBy("age", "name").mapToRows { group.first() }
— sort rows by key columns   — reorder rows randomly   — reverse order of rows
Returns DataFrame with rows sorted by one or several columns.  By default, columns are sorted in ascending order with null values going first. Available modifiers:   .desc — changes column sort order from ascending to descending  .nullsLast — forces null values to be placed at the end of the order       df.sortBy { age } df.sortBy { age and name.firstName.desc() } df.sortBy { weight.nullsLast() }    val age by column<Int>() val weight by column<Int?>() val name by columnGroup() val firstName by name.column<String>() df.sortBy { age } df.sortBy { age and firstName } df.sortBy { weight.nullsLast() }    df.sortBy("age") df.sortBy { "age" and "name"["firstName"].desc() } df.sortBy { "weight".nullsLast() }      Returns DataFrame sorted by one or several columns in descending order.      df.sortByDesc { age and weight }    val age by column<Int>() val weight by column<Int?>() df.sortByDesc { age and weight }    df.sortByDesc("age", "weight")      Returns DataFrame sorted with comparator.    df.sortWith { row1, row2 -> when { row1.age < row2.age -> -1 row1.age > row2.age -> 1 else -> row1.name.firstName.compareTo(row2.name.firstName) } }
Returns DataFrame with randomly reordered rows.    df.shuffle()
Returns DataFrame with rows in reversed order.    df.reverse()
— groups rows of DataFrame by given key columns   — concatenates rows from several DataFrames into single DataFrame
Splits the rows of DataFrame into groups using one or several columns as grouping keys.  groupBy { columns } [ transformations ] reducer | aggregator | pivot transformations = [ .sortByCount() | .sortByCountAsc() | .sortBy { columns } | .sortByDesc { columns } ] [ .updateGroups { frameExpression } ] [ .add(column) { rowExpression } ] reducer = .minBy { column } | .maxBy { column } | .first [ { rowCondition } ] | .last [ { rowCondition } ] .concat() | .into([column]) [{ rowExpression }] | .values { valueColumns } aggregator = .count() | .concat() | .into([column]) [{ rowExpression }] | .values { valueColumns } | .aggregate { aggregations } | .<stat> [ { columns } ] pivot = .pivot { columns } [ .default(defaultValue) ] pivotReducer | pivotAggregator  See  ,  ,  ,       df.groupBy { name } df.groupBy { city and name.lastName } df.groupBy { age / 10 named "ageDecade" } df.groupBy { expr { name.firstName.length + name.lastName.length } named "nameLength" }    val name by columnGroup() val lastName by name.column<String>() val firstName by name.column<String>() val age by column<Int>() val city by column<String?>() df.groupBy { name } // or df.groupBy(name) df.groupBy { city and lastName } // or df.groupBy(city, lastName) df.groupBy { age / 10 named "ageDecade" } df.groupBy { expr { firstName().length + lastName().length } named "nameLength" }    df.groupBy("name") df.groupBy { "city" and "name"["lastName"] } df.groupBy { "age"<Int>() / 10 named "ageDecade" } df.groupBy { expr { "name"["firstName"]<String>().length + "name"["lastName"]<String>().length } named "nameLength" }     Returns GroupBy object.   GroupBy is a DataFrame with one chosen  containing data groups.  It supports the following operations:              Any DataFrame with FrameColumn can be reinterpreted as GroupBy :    val key by columnOf(1, 2) // create int column with name "key" val data by columnOf(df[0..3], df[4..6]) // create frame column with name "data" val df = dataFrameOf(key, data) // create dataframe with two columns df.asGroupBy { data } // convert dataframe to GroupBy by interpreting 'data' column as groups    And any GroupBy can be reinterpreted as DataFrame with FrameColumn :    df.groupBy { city }.toDataFrame()    Use  to union all data groups of GroupBy into original DataFrame preserving new order of rows produced by grouping:    df.groupBy { name }.concat()     To compute one or several  per every group of GroupBy use aggregate function. Its body will be executed for every data group and has a receiver of type DataFrame that represents current data group being aggregated. To add a new column to the resulting DataFrame , pass the name of new column to infix function into :      df.groupBy { city }.aggregate { count() into "total" count { age > 18 } into "adults" median { age } into "median age" min { age } into "min age" maxBy { age }.name into "oldest" }    val city by column<String?>() val age by column<Int>() val name by columnGroup() df.groupBy { city }.aggregate { count() into "total" count { age() > 18 } into "adults" median { age } into "median age" min { age } into "min age" maxBy { age() }[name] into "name of oldest" } // or df.groupBy(city).aggregate { count() into "total" count { age > 18 } into "adults" median(age) into "median age" min(age) into "min age" maxBy(age)[name] into "name of oldest" } // or df.groupBy(city).aggregate { count() into "total" age().count { it > 18 } into "adults" age().median() into "median age" age().min() into "min age" maxBy(age)[name] into "name of oldest" }    df.groupBy("city").aggregate { count() into "total" count { "age"<Int>() > 18 } into "adults" median("age") into "median age" min("age") into "min age" maxBy("age")["name"] into "oldest" } // or df.groupBy("city").aggregate { count() into "total" count { "age"<Int>() > 18 } into "adults" "age"<Int>().median() into "median age" "age"<Int>().min() into "min age" maxBy("age")["name"] into "oldest" }     If only one aggregation function is used, column name can be omitted:      df.groupBy { city }.aggregate { maxBy { age }.name }    val city by column<String?>() val age by column<Int>() val name by columnGroup() df.groupBy { city }.aggregate { maxBy { age() }[name] } // or df.groupBy(city).aggregate { maxBy(age)[name] }    df.groupBy("city").aggregate { maxBy("age")["name"] }     Most common aggregation functions can be computed directly at GroupBy :      df.groupBy { city }.max() // max for every comparable column df.groupBy { city }.mean() // mean for every numeric column df.groupBy { city }.max { age } // max age into column "age" df.groupBy { city }.sum("total weight") { weight } // sum of weights into column "total weight" df.groupBy { city }.count() // number of rows into column "count" df.groupBy { city } .max { name.firstName.length() and name.lastName.length() } // maximum length of firstName or lastName into column "max" df.groupBy { city } .medianFor { age and weight } // median age into column "age", median weight into column "weight" df.groupBy { city } .minFor { (age into "min age") and (weight into "min weight") } // min age into column "min age", min weight into column "min weight" df.groupBy { city }.meanOf("mean ratio") { weight?.div(age) } // mean of weight/age into column "mean ratio"    val city by column<String?>() val age by column<Int>() val weight by column<Int?>() val name by columnGroup() val firstName by name.column<String>() val lastName by name.column<String>() df.groupBy { city }.max() // max for every comparable column df.groupBy { city }.mean() // mean for every numeric column df.groupBy { city }.max { age } // max age into column "age" df.groupBy { city }.sum("total weight") { weight } // sum of weights into column "total weight" df.groupBy { city }.count() // number of rows into column "count" df.groupBy { city } .max { firstName.length() and lastName.length() } // maximum length of firstName or lastName into column "max" df.groupBy { city } .medianFor { age and weight } // median age into column "age", median weight into column "weight" df.groupBy { city } .minFor { (age into "min age") and (weight into "min weight") } // min age into column "min age", min weight into column "min weight" df.groupBy { city }.meanOf("mean ratio") { weight()?.div(age()) } // mean of weight/age into column "mean ratio"    df.groupBy("city").max() // max for every comparable column df.groupBy("city").mean() // mean for every numeric column df.groupBy("city").max("age") // max age into column "age" df.groupBy("city").sum("weight", name = "total weight") // sum of weights into column "total weight" df.groupBy("city").count() // number of rows into column "count" df.groupBy("city").max { "name"["firstName"]<String>().length() and "name"["lastName"]<String>().length() } // maximum length of firstName or lastName into column "max" df.groupBy("city") .medianFor("age", "weight") // median age into column "age", median weight into column "weight" df.groupBy("city") .minFor { ("age"<Int>() into "min age") and ("weight"<Int?>() into "min weight") } // min age into column "min age", min weight into column "min weight" df.groupBy("city").meanOf("mean ratio") { "weight"<Int?>()?.div("age"<Int>()) } // mean of weight/age into column "mean ratio"     To get all column values for every group without aggregation use values function:   for  of type T it will gather group values into lists of type List<T>  for  it will gather group values into DataFrame and convert  into        df.groupBy { city }.values() df.groupBy { city }.values { name and age } df.groupBy { city }.values { weight into "weights" }    val city by column<String?>() val age by column<Int>() val weight by column<Int?>() val name by columnGroup() df.groupBy(city).values() df.groupBy(city).values(name, age) df.groupBy(city).values { weight into "weights" }    df.groupBy("city").values() df.groupBy("city").values("name", "age") df.groupBy("city").values { "weight" into "weights" }
Returns DataFrame with the union of rows from several given DataFrames .  concat is available for:  DataFrame :    df.concat(df1, df2)    DataColumn :    val a by columnOf(1, 2) val b by columnOf(3, 4) a.concat(b)    Iterable<DataFrame> :    listOf(df1, df2).concat()    Iterable<DataRow> :    val rows = listOf(df[2], df[4], df[5]) rows.concat()    Iterable<DataColumn> :    val a by columnOf(1, 2) val b by columnOf(3, 4) listOf(a, b).concat()     :    df.groupBy { name }.concat()     :    val x = dataFrameOf("a", "b")( 1, 2, 3, 4 ) val y = dataFrameOf("b", "c")( 5, 6, 7, 8 ) val frameColumn by columnOf(x, y) frameColumn.concat()    If you want to union columns (not rows) from several DataFrames , see  .   If input DataFrames have different schemas, every column in resulting DataFrame will have the most common type of the original columns with the same name.  For example, if one DataFrame has column A: Int and other DataFrame has column A: Double , resulting DataFrame will have column A: Number .  Missing columns in dataframes will be filled with null .
Both  and  can be used to change columns values in DataFrame .  Difference between these operations:   convert allows to change the type of the column, update doesn't  update allows to filter cells to be updated, convert doesn't
Returns DataFrame with changed values in some cells. Column types can not be changed.  update { columns } [.where { rowCondition } ] [.at(rowIndices) ] .with { rowExpression } | .notNull { rowExpression } | .perCol { colExpression } | .perRowCol { rowColExpression } | .withValue(value) | .withNull() | .withZero() | .asFrame { frameExpression } rowCondition: DataRow.(OldValue) -> Boolean rowExpression: DataRow.(OldValue) -> NewValue colExpression: DataColumn.(DataColumn) -> NewValue rowColExpression: DataRow.(DataColumn) -> NewValue frameExpression: DataFrame.(DataFrame) -> DataFrame  See  and     df.update { age }.with { it * 2 } df.update { dfsOf<String>() }.with { it.uppercase() } df.update { weight }.at(1..4).notNull { it / 2 } df.update { name.lastName and age }.at(1, 3, 4).withNull()    Update with constant value:    df.update { city }.where { name.firstName == "Alice" }.withValue("Paris")    Update with value depending on row:    df.update { city }.with { name.firstName + " from " + it }    Update with value depending on column:    df.update { colsOf<Number?>() }.perCol { mean(skipNA = true) }    Update with value depending on row and column:    df.update { colsOf<String?>() }.perRowCol { row, col -> col.name() + ": " + row.index() }    Update  as  :    df.update { name }.asFrame { select { lastName } }
Replace missing values.   Replaces null values with given value or expression.    df.fillNulls { colsOf<Int?>() }.with { -1 } // same as df.update { colsOf<Int?>() }.where { it == null }.with { -1 }     Replaces Double.NaN and Float.NaN values with given value or expression.    df.fillNaNs { colsOf<Double>() }.withZero()     Replaces null , Double.NaN and Float.NaN values with given value or expression.    df.fillNA { weight }.withValue(-1)
Returns DataFrame with changed values in some columns. Allows to change column types.  convert { columnsSelector } .with { rowExpression } | .perRowCol { rowColExpression } | .withValue(value) | to<Type>() | to { colExpression } rowExpression = DataRow.(OldValue) -> NewValue rowColExpression = DataRow.(DataColumn) -> NewValue colExpression = DataFrame.(DataColumn) -> DataColumn  See  and     df.convert { age }.with { it.toDouble() } df.convert { dfsOf<String>() }.with { it.toCharArray().toList() }    convert supports automatic type conversions between the following types:   Int  String  Double  Long  Short  Float  BigDecimal  LocalDateTime  LocalDate  LocalTime  Duration     df.convert { age }.to<Double>() df.convert { colsOf<Number>() }.to<String>() df.convert { name.firstName and name.lastName }.to { it.length() } df.convert { weight }.toFloat()    Automatic conversion from String into enum class is also supported:  enum class Direction { NORTH, SOUTH, WEST, EAST }    dataFrameOf("direction")("NORTH", "WEST") .convert("direction").to<Direction>()
Returns DataFrame in which given String columns are parsed into other types.  Special case of  operation.    df.parse()    To parse only particular columns use  :    df.parse { age and weight }    parse tries to parse every String column into one of supported types in the following order:   Int  Long  LocalDateTime  LocalDate  LocalTime  URL  Double  Boolean  BigDecimal   Available parser options:   locale: Locale is used to parse numbers  dateTimePattern: String is used to parse date and time  dateTimeFormatter: DateTimeFormatter is used to parse date and time  nullStrings: List<String> is used to treat particular strings as null value. Default null strings are "null" and "NULL"     df.parse(options = ParserOptions(locale = Locale.CHINA, dateTimeFormatter = DateTimeFormatter.ISO_WEEK_DATE))    You can also set global parser options that will be used by default in  ,  and parse operations:    DataFrame.parser.locale = Locale.FRANCE DataFrame.parser.addDateTimePattern("dd.MM.uuuu HH:mm:ss")
Changes type of the selected columns based on actual values stored in these columns. Resulting type of the column will be a nearest common supertype of all column values.  inferType [ { columns } ]
column values horizontally or vertically   values from several columns into single column
Splits every value in the given columns into several values and optionally spreads them horizontally or vertically.  df.split { columns } [.cast<Type>()] [.by(delimeters) | .by { splitter } | .match(regex)] // how to split cell value [.default(value)] // how to fill nulls .into(columnNames) [ { columnNamesGenerator } ] | .inward(columnNames) [ { columnNamesGenerator } | .inplace() | .intoRows() | .intoColumns() ] // where to store results splitter = DataRow.(T) -> Iterable<Any> columnNamesGenerator = DataColumn.(columnIndex: Int) -> String  The following types of columns can be split without any splitter configuration:   String : split by , and trim  List : split into elements  DataFrame : split into rows    Stores split values as lists in original columns.  Use .inplace() terminal operation in split configuration to spread split values inplace:      df.split { name.firstName }.by { it.chars().toList() }.inplace()    val name by columnGroup() val firstName by name.column<String>() df.split { firstName }.by { it.chars().toList() }.inplace()    df.split { "name"["firstName"]<String>() }.by { it.chars().toList() }.inplace()      Stores split values in new columns.   into(col1, col2, ... ) — store splitted values in new top-level columns  inward(col1, col2, ...) — store splitted values in new columns nested inside original column  intoColumns — split FrameColumn into ColumnGroup storing in every cell a List of original values per every column   Reverse operation:    columnNamesGenerator is used to generate names for additional columns when the list of explicitly specified columnNames was not long enough. columnIndex starts with 1 for the first additional column name.  Default columnNamesGenerator generates column names splitted1 , splitted2 ...      df.split { name }.by { it.values() }.into("nameParts") df.split { name.lastName }.by(" ").default("").inward { "word$it" }    val name by columnGroup() val lastName by name.column<String>() df.split { name }.by { it.values() }.into("nameParts") df.split { lastName }.by(" ").default("").inward { "word$it" }    df.split { name }.by { it.values() }.into("nameParts") df.split { "name"["lastName"] }.by(" ").default("").inward { "word$it" }     String columns can also be splitted into group matches of  pattern:    merged.split { name } .match("""(.*) \((.*)\)""") .inward("firstName", "lastName")    FrameColumn can be splitted into columns:    val df1 = dataFrameOf("a", "b", "c")( 1, 2, 3, 4, 5, 6 ) val df2 = dataFrameOf("a", "b")( 5, 6, 7, 8, 9, 10 ) val group by columnOf(df1, df2) val id by columnOf("x", "y") val df = dataFrameOf(id, group) df.split { group }.intoColumns()     Stores split values in new rows duplicating values in other columns.  Reverse operation:    Use .intoRows() terminal operation in split configuration to spread split values vertically:      df.split { name.firstName }.by { it.chars().toList() }.intoRows() df.split { name }.by { it.values() }.intoRows()    val name by columnGroup() val firstName by name.column<String>() df.split { firstName }.by { it.chars().toList() }.intoRows() df.split { name }.by { it.values() }.intoRows()    df.split { "name"["firstName"]<String>() }.by { it.chars().toList() }.intoRows() df.split { group("name") }.by { it.values() }.intoRows()     Equals to split { column }...inplace().explode { column } . See  for details.
Merges several columns into a single column.  Reverse operation to   merge { columns } [.notNull()] .by(delimeter) | .by { merger } [.into(column) | .intoList() ] merger: (DataRow).List<T> -> Any    // Merge two columns into one column "fullName" df.merge { name.firstName and name.lastName }.by(" ").into("fullName")    merger accepts a List of collected values for every row typed by their common type:    df.merge { name.firstName and name.lastName } .by { it[0] + " (" + it[1].uppercase() + ")" } .into("fullName")    When heterogeneous columns are merged, they may need to be cast to valid types in merger :    df.merge { name.firstName and age and isHappy } .by { "${it[0]} aged ${it[1]} is " + (if (it[2] as Boolean) "" else "not ") + "happy" } .into("status")    By default, when no delimeter or merger is specified, values will be merged into the List :    df.merge { colsOf<Number>() }.into("data")    Merged column values can also be exported to List :    // Merge data from two columns into List<String> df.merge { name.firstName and name.lastName }.by(",").intoList()
columns to DataFrame   columns to new DataFrame or DataColumn   columns from DataFrame
Returns DataFrame which contains all columns from original DataFrame followed by newly added columns. Original DataFrame is not modified.  Create new column and add it to DataFrame :  add(columnName: String) { rowExpression } rowExpression: DataRow.(DataRow) -> Value      df.add("year of birth") { 2021 - age }    val age by column<Int>() val yearOfBirth by column<Int>("year of birth") df.add(yearOfBirth) { 2021 - age }    df.add("year of birth") { 2021 - "age"<Int>() }     See   You can use newValue() function to access value that was already calculated for preceding row. It is helpful for recurrent computations:    df.add("fibonacci") { if (index() < 2) 1 else prev()!!.newValue<Int>() + prev()!!.prev()!!.newValue<Int>() }    Create and add several columns to DataFrame :  add { columnMapping columnMapping ... } columnMapping = column into columnName | columnName from column | columnName from { rowExpression }      df.add { "year of birth" from 2021 - age age gt 18 into "is adult" "details" { name.lastName.length() into "last name length" "full name" from { name.firstName + " " + name.lastName } } }    val yob = column<Int>("year of birth") val lastNameLength = column<Int>("last name length") val age by column<Int>() val isAdult = column<Boolean>("is adult") val fullName = column<String>("full name") val name by columnGroup() val details by columnGroup() val firstName by name.column<String>() val lastName by name.column<String>() df.add { yob from 2021 - age age gt 18 into isAdult details from { lastName.length() into lastNameLength fullName from { firstName() + " " + lastName() } } }    df.add { "year of birth" from 2021 - "age"<Int>() "age"<Int>() gt 18 into "is adult" "details" { "name"["lastName"]<String>().length() into "last name length" "full name" from { "name"["firstName"]<String>() + " " + "name"["lastName"]<String>() } } }     Add existing column to DataFrame :    val score by columnOf(4, 3, 5, 2, 1, 3, 5) df.add(score) df + score    Add all columns from another DataFrame :    df.add(df1, df2)     Adds column with sequential values 0, 1, 2,... New column will be added in the beginning of columns list and will become the first column in DataFrame .  addId(name: String = "id")  Parameters:   name: String = "id" - name of the new column.
Creates List ,  or  with values computed from rows of original DataFrame .  Map into List :  map { rowExpression }: List<T> rowExpression: DataRow.(DataRow) -> Value    df.map { 2021 - it.age }    Map into DataColumn :  mapToColumn(columnName) { rowExpression }: DataColumn rowExpression: DataRow.(DataRow) -> Value      df.mapToColumn("year of birth") { 2021 - age }    val age by column<Int>() val yearOfBirth by column<Int>("year of birth") df.mapToColumn(yearOfBirth) { 2021 - age }    df.mapToColumn("year of birth") { 2021 - "age"<Int>() }     See   Map into DataFrame :  mapToFrame { columnMapping columnMapping ... } : DataFrame columnMapping = column into columnName | columnName from column | columnName from { rowExpression } | +column      df.mapToFrame { "year of birth" from 2021 - age age gt 18 into "is adult" name.lastName.length() into "last name length" "full name" from { name.firstName + " " + name.lastName } +city }    val yob = column<Int>("year of birth") val lastNameLength = column<Int>("last name length") val age by column<Int>() val isAdult = column<Boolean>("is adult") val fullName = column<String>("full name") val name by columnGroup() val firstName by name.column<String>() val lastName by name.column<String>() val city by column<String?>() df.mapToFrame { yob from 2021 - age age gt 18 into isAdult lastName.length() into lastNameLength fullName from { firstName() + " " + lastName() } +city }    df.mapToFrame { "year of birth" from 2021 - "age"<Int>() "age"<Int>() gt 18 into "is adult" "name"["lastName"]<String>().length() into "last name length" "full name" from { "name"["firstName"]<String>() + " " + "name"["lastName"]<String>() } +"city" }
Returns DataFrame without selected columns.  remove { columns }  See       df.remove { name and weight }    val name by columnGroup() val weight by column<Int?>() df.remove { name and weight }    df.remove("name", "weight")
— move columns or change column grouping   — rename columns   — reorder columns
Moves one or several columns within DataFrame .  move { columns } .into { pathSelector } | .under { parentColumn } | .after { column } | .to(position) | .toTop() | .toLeft() | .toRight() pathSelector: DataFrame.(DataColumn) -> ColumnPath  See   Can be used to change columns hierarchy by providing ColumnPath for every moved column    df.move { age }.toLeft() df.move { weight }.to(1) // age -> info.age // weight -> info.weight df.move { age and weight }.into { pathOf("info", it.name()) } df.move { age and weight }.into { "info"[it.name()] } df.move { age and weight }.under("info") // name.firstName -> fullName.first // name.lastName -> fullName.last df.move { name.firstName and name.lastName }.into { pathOf("fullName", it.name().dropLast(4)) } // a|b|c -> a.b.c // a|d|e -> a.d.e dataFrameOf("a|b|c", "a|d|e")(0, 0) .move { all() }.into { it.name().split("|").toPath() } // name.firstName -> firstName // name.lastName -> lastName df.move { name.cols() }.toTop() // a.b.e -> be // c.d.e -> de df.move { dfs { it.name() == "e" } }.toTop { it.parentName + it.name() }    Special cases of move :    — groups columns into    — ungroups    — removes all column groupings
Renames one or several columns without changing its location in DataFrame  df.rename { columns }.into(name) df.rename { columns }.into { nameExpression } nameExpression = (DataColumn) -> String
Returns DataFrame with a new order of selected columns.  reorder { columns } [.cast<ColumnType>() ] .by { columnExpression } | .byDesc { columnExpression } | .byName(desc = false) { columnExpression } columnExpression: DataColumn.(DataColumn) -> Value      df.reorder { age..isHappy }.byName()    val age by column<Int>() val isHappy by column<Boolean>() df.reorder { age..isHappy }.byName()    df.reorder { "age".."isHappy" }.byName() }     When a subset of columns is selected they will be reordered among their original positions. Positions of other columns will not change.  If selected columns belong to different column groups they will be reordered within their groups, so column grouping will be preserved.    val df = dataFrameOf("c", "d", "a", "b")( 3, 4, 1, 2, 1, 1, 1, 1 ) df.reorder("d", "b").cast<Int>().by { sum() } // [c, b, a, d]    When exactly one ColumnGroup is selected, reordering is applied to its nested columns.    df.reorder { name }.byName(desc = true) // [name.lastName, name.firstName]     Reorders all columns  reorderColumnsBy(dfs = true, desc = false) { columnExpression }  Parameters:   dfs — reorder columns inside ColumnGroups and FrameColumns recursively  desc — apply descending order    reorderColumnsByName(dfs = true, desc = false)  Parameters:   dfs — reorder columns inside ColumnGroups and FrameColumns recursively  desc — apply descending order
— groups given columns into  .   — ungroups given  by replacing them with their children columns   — recursively removes all column groupings under given  , remaining only  and    These operations are special cases of general  operation.
Group columns into  .  group { columns } .into(groupName) | .into { groupNameExpression } groupNameExpression = DataColumn.(DataColumn) -> String  Reverse operation:    It is a special case of  operation.    df.group { age and city }.into("info") df.group { all() }.into { it.type().toString() }.print()
Replaces ColumnGroup with its nested columns.  ungroup { columns }  Reverse operation:    See     // name.firstName -> firstName // name.lastName -> lastName df.ungroup { name }
Returns DataFrame without column groupings under selected columns  flatten [ { columns } ]  Columns after flattening will keep their original names. Potential column name clashes are resolved by adding minimal possible name prefix from ancestor columns.    // name.firstName -> firstName // name.lastName -> lastName df.flatten { name }    To remove all column groupings in DataFrame , invoke flatten without parameters:    df.flatten()
— inserts new column into DataFrame   — replaces columns in DataFrame
Inserts new column at specific position in DataFrame .  insert (columnName) { rowExpression } | (column) .under { parentColumn } | .after { column } | .at(position) rowExpression: DataRow.(DataRow) -> Value  Similar to  , but supports column positioning.  Create new column based on existing columns and insert it into DataFrame :      df.insert("year of birth") { 2021 - age }.after { age }    val year = column<Int>("year of birth") val age by column<Int>() df.insert(year) { 2021 - age }.after { age }    df.insert("year of birth") { 2021 - "age"<Int>() }.after("age")     Insert previously created column:    val score by columnOf(4, 5, 3, 5, 4, 5, 3) df.insert(score).at(2)
Replaces one or several columns with new columns.  replace { columns } .with(newColumns) | .with { columnExpression } columnExpression: DataFrame.(DataColumn) -> DataColumn  See     df.replace { name }.with { name.firstName } df.replace { colsOf<String?>() }.with { it.lowercase() } df.replace { age }.with { 2021 - age named "year" }    `replace { columns }.with { columnExpression } ` is equivalent to `convert { columns }.to { columnExpression }`. See [`convert`](convert.md) for details.
— distributes lists of values or dataframes in given columns vertically, replicating data in other columns   — collects column values in given columns into lists or dataframes, grouping by other columns
Splits list-like values in given columns and spreads them vertically. Values in other columns are duplicated.  explode(dropEmpty = true) [ { columns } ]  Parameters:   dropEmpty — if true , removes rows with empty lists or dataframes. Otherwise, they will be exploded into null .   Available for:   DataFrame  FrameColumn  DataColumn<Collection>   Reverse operation:    Exploded columns will change their types:   List<T> to T  DataFrame to DataRow   Exploded  will be converted into  .  Explode DataFrame :      val a by columnOf(1, 2) val b by columnOf(listOf(1, 2), listOf(3, 4)) val df = dataFrameOf(a, b) df.explode { b }    val df = dataFrameOf("a", "b")( 1, listOf(1, 2), 2, listOf(3, 4) ) df.explode("b")     When several columns are exploded in one operation, lists in different columns will be aligned.    val a by columnOf(listOf(1, 2), listOf(3, 4, 5)) val b by columnOf(listOf(1, 2, 3), listOf(4, 5)) val df = dataFrameOf(a, b) df.explode { a and b }    Explode DataColumn<Collection> :    val col by columnOf(listOf(1, 2), listOf(3, 4)) col.explode()    Explode FrameColumn :    val col by columnOf( dataFrameOf("a", "b")(1, 2, 3, 4), dataFrameOf("a", "b")(5, 6, 7, 8) ) col.explode()
Returns DataFrame where values in given columns are merged into lists grouped by other columns.  implode(dropNA = false) [ { columns } ]  Parameters:   dropNA — if true , removes NA values from merged lists.   Reverse operation:    Imploded columns will change their types:   T to List<T>  DataRow to DataFrame   Imploded  will convert into     df.implode { name and age and weight and isHappy }
— transforms column values into new columns (long to wide)   — collects values from several columns into two key and value columns (wide to long)
Splits the rows of DataFrame and groups them horizontally into new columns based on values from one or several columns of original DataFrame .  pivot (inward = true) { pivotColumns } [ .groupBy { indexColumns } | .groupByOther() ] [ .default(defaultValue) ] reducer | aggregator reducer = .minBy { column } | .maxBy { column } | .first [ { rowCondition } ] | .last [ { rowCondition } ] .with { rowExpression } | .values { valueColumns } aggregator = .count() | .matches() | .frames() | .with { rowExpression } | .values { valueColumns } | .aggregate { aggregations } | .<stat> [ { columns } ]  Parameters:   inward — if true generated columns will be nested inside original column, otherwise they will be top-level  pivotColumns — columns with values for horizontal data grouping and generation of new columns  indexColumns — columns with values for vertical data grouping  defaultValue — value to fill mismatched pivot-index column pairs  valueColumns — columns with output values       df.pivot { city }    val city by column<String?>() df.pivot { city }    df.pivot("city")     To pivot several columns at once you can combine them using and or then infix function:   and will pivot columns independently  then will create column hierarchy from combinations of values from pivoted columns       df.pivot { city and name.firstName } df.pivot { city then name.firstName }    val city by column<String?>() val name by columnGroup() val firstName by name.column<String>() df.pivot { city and firstName } df.pivot { city then firstName }    df.pivot { "city" and "name"["firstName"] } df.pivot { "city" then "name"["firstName"] }      To create matrix table that is expanded both horizontally and vertically, apply groupBy transformation passing the columns for vertical grouping. Reversed order of pivot and groupBy will produce the same result.      df.pivot { city }.groupBy { name } // same as df.groupBy { name }.pivot { city }    val city by column<String?>() val name by columnGroup() df.pivot { city }.groupBy { name } // same as df.groupBy { name }.pivot { city }    df.pivot("city").groupBy("name") // same as df.groupBy("name").pivot("city")     To group by all columns except pivoted use groupByOther :    df.pivot { city }.groupByOther()     To aggregate data groups with one or several statistics use aggregate :      df.pivot { city }.aggregate { minBy { age }.name } df.pivot { city }.groupBy { name.firstName }.aggregate { meanFor { age and weight } into "means" stdFor { age and weight } into "stds" maxByOrNull { weight }?.name?.lastName into "biggest" }    val city by column<String?>() val name by columnGroup() val firstName by name.column<String>() val age by column<Int>() val weight by column<Int?>() df.pivot { city }.aggregate { minBy(age).name } df.pivot { city }.groupBy { firstName }.aggregate { meanFor { age and weight } into "means" stdFor { age and weight } into "stds" maxByOrNull(weight)?.name?.lastName into "biggest" }    df.pivot("city").aggregate { minBy("age")["name"] } df.pivot("city").groupBy { "name"["firstName"] }.aggregate { meanFor("age", "weight") into "means" stdFor("age", "weight") into "stds" maxByOrNull("weight")?.getColumnGroup("name")?.get("lastName") into "biggest" }     Shortcuts for common aggregation functions are also available:      df.pivot { city }.maxFor { age and weight } df.groupBy { name }.pivot { city }.median { age }    val city by column<String?>() val name by columnGroup() val age by column<Int>() val weight by column<Int?>() df.pivot { city }.maxFor { age and weight } df.groupBy { name }.pivot { city }.median { age }    df.pivot("city").maxFor("age", "weight") df.groupBy("name").pivot("city").median("age")     By default, when aggregation function produces several values for single data group, column hierarchy in resulting DataFrame will be indexed first by pivot keys and then by the names of aggregated values. To reverse this order so that resulting columns will be indexed first by names of aggregated values and then by pivot keys, use separate=true flag that is available in multi-result aggregation operations, such as aggregate or <stat>For :      df.pivot { city }.maxFor(separate = true) { age and weight } df.pivot { city }.aggregate(separate = true) { min { age } into "min age" maxOrNull { weight } into "max weight" }    val city by column<String?>() val age by column<Int>() val weight by column<Int?>() df.pivot { city }.maxFor(separate = true) { age and weight } df.pivot { city }.aggregate(separate = true) { min { age } into "min age" maxOrNull { weight } into "max weight" }    df.pivot("city").maxFor("age", "weight", separate = true) df.pivot("city").aggregate(separate = true) { min("age") into "min age" maxOrNull("weight") into "max weight" }     By default, any aggregation function will result in null value for those matrix cells, where intersection of column and row keys produced an empty data group. You can specify default value for any aggregation by default infix function. This value will replace all null results of aggregation function over non-empty data groups as well. To use one default value for all aggregation functions, use default() before aggregation.      df.pivot { city }.groupBy { name }.aggregate { min { age } default 0 } df.pivot { city }.groupBy { name }.aggregate { median { age } into "median age" default 0 minOrNull { weight } into "min weight" default 100 } df.pivot { city }.groupBy { name }.default(0).min()    val city by column<String?>() val age by column<Int>() val weight by column<Int?>() val name by columnGroup() df.pivot { city }.groupBy { name }.aggregate { min { age } default 0 } df.pivot { city }.groupBy { name }.aggregate { median { age } into "median age" default 0 minOrNull { weight } into "min weight" default 100 } df.pivot { city }.groupBy { name }.default(0).min()    df.pivot("city").groupBy("name").aggregate { min("age") default 0 } df.pivot("city").groupBy("name").aggregate { median("age") into "median age" default 0 minOrNull("weight") into "min weight" default 100 } df.pivot("city").groupBy("name").default(0).min()       transformation can be used inside  function of  . This allows to combine column pivoting with other  aggregations:      df.groupBy { name.firstName }.aggregate { pivot { city }.aggregate(separate = true) { mean { age } into "mean age" count() into "count" } count() into "total" }    val city by column<String?>() val name by columnGroup() val firstName by name.column<String>() val age by column<Int>() df.groupBy { firstName }.aggregate { pivot { city }.aggregate(separate = true) { mean { age } into "mean age" count() into "count" } count() into "total" }    df.groupBy { "name"["firstName"] }.aggregate { pivot("city").aggregate(separate = true) { mean("age") into "mean age" count() into "count" } count() into "total" }      Pivots with  statistics one or several columns preserving all other columns of DataFrame or GroupBy .    df.pivotCounts { city } // same as df.pivot { city }.groupByOther().count() df.groupBy { name }.pivotCounts { city } // same as df.groupBy { name }.pivot { city }.count() // same as df.groupBy { name }.aggregate { pivotCounts { city } }     Pivots with Boolean statistics one or several columns preserving all other columns of DataFrame .    df.pivotMatches { city } // same as df.pivot { city }.groupByOther().matches() df.groupBy { name }.pivotMatches { city } // same as df.groupBy { name }.pivot { city }.matches() // same as df.groupBy { name }.aggregate { pivotMatches { city } }
Converts several columns into two columns key and value . key column will contain names of original columns, value column will contain values from original columns.  This operation is reverse to   gather { columns } [.explodeLists()] [.cast<Type>()] [.notNull()] [.where { valueFilter }] [.mapKeys { keyTransform }] [.mapValues { valueTransform }] .into(keyColumn, valueColumn) | .keysInto(keyColumn) | .valuesInto(valueColumn) valueFilter: (value) -> Boolean keyTransform: (columnName: String) -> K valueTransform: (value) -> R  See   Configuration options:   explodeLists — gathered values of type List will be exploded into their elements, so where , cast , notNull and mapValues will be applied to list elements instead of lists themselves  cast — inform compiler about the expected type of gathered elements. This type will be passed to where and mapKeys lambdas  notNull — skip gathered null values  where — filter gathered values  mapKeys — transform gathered column names (keys)  mapValues — transform gathered column values   Storage options:   into(keyColumn, valueColumn) — store gathered key-value pairs in two new columns with names keyColumn and valueColumn  keysInto(keyColumn) — store only gathered keys (column names) in a new column keyColumn  valuesInto(valueColumn) — store only gathered values in a new column valueColumn     pivoted.gather { "London".."Tokyo" }.into("city", "population")      pivoted.gather { "London".."Tokyo" } .cast<Int>() .where { it > 10 } .mapKeys { it.lowercase() } .mapValues { 1.0 / it } .into("city", "density")
— append new rows   — duplicate selected rows
Adds one or several rows to DataFrame  df.append ( "Mike", 15, "John", 17, "Bill", 30)
Returns  with original  repeated n times.  DataRow.duplicate(n): DataFrame  Returns  with original  repeated n times. Resulting FrameColumn will have an empty  .  DataFrame.duplicate(n): FrameColumn  Returns  where rows that satisfy to the given  are repeated n times. If rowCondition is not specified all rows will be duplicated.  DataFrame.duplicateRows(n) [ { rowCondition } ]: DataFrame
DataFrame interface has type argument T that doesn't affect contents of DataFrame , but marks DataFrame with a type that represents data schema that this DataFrame is supposed to have. This argument is used to generate  for typed data access.  Actual data in DataFrame may diverge from compile-time schema marker T due to dynamic nature of data inside DataFrame . However, at some points of code you may know exactly what DataFrame schema is expected. To match your knowledge with expected real-time DataFrame contents you can use one of two functions:    — change type argument of DataFrame to the expected schema without changing data in DataFrame .   — convert DataFrame contents to match the expected schema.
Changes type argument of DataFrame without changing its contents.  cast<T>(verify = false)  Parameters:   verify: Boolean = false — when true , throws exception if DataFrame doesn't match given schema. Otherwise, just changes format type without actual data check.   Use this operation to change formal type of DataFrame to match expected schema and enable generated  for it.  @DataSchema interface Person { val age: Int val name: String } df.cast<Person>()  To convert DataFrame columns to match given schema, use  operation.
columns in DataFrame to match given schema.  convertTo<T>()  Any additional columns will be dropped.
Every summary statistics can be used in aggregations of:                df.mean() df.age.sum() df.groupBy { city }.mean() df.pivot { city }.median() df.pivot { city }.groupBy { name.lastName }.std()     ,  ,  are available for numeric columns of types Int , Double , Float , BigDecimal , Long , Byte .   ,  are available for Comparable columns.  When statistics x is applied to several columns, it can be computed in several modes:   x(): DataRow computes separate value per every suitable column  x { columns }: Value computes single value across all given columns  xFor { columns }: DataRow computes separate value per every given column  xOf { rowExpression }: Value computes single value across results of  evaluated for every row    and  statistics have additional mode by :   minBy { rowExpression }: DataRow finds a row with minimal result of      df.sum() // sum of values per every numeric column df.sum { age and weight } // sum of all values in `age` and `weight` df.sumFor { age and weight } // sum of values per `age` and `weight` separately df.sumOf { (weight ?: 0) / age } // sum of expression evaluated for every row     When statistics is applied to GroupBy , it is computed for every data group.  If statistic is applied in a mode that returns a single value for every data group, it will be stored in a single column named by statistic name.    df.groupBy { city }.mean { age } // [`city`, `mean`] df.groupBy { city }.meanOf { age / 2 } // [`city`, `mean`]    You can also pass custom name for aggregated column:    df.groupBy { city }.mean("mean age") { age } // [`city`, `mean age`] df.groupBy { city }.meanOf("custom") { age / 2 } // [`city`, `custom`]    If statistic is applied in a mode that returns separate value per every column in data group, aggregated values will be stored in columns with original column names.    df.groupBy { city }.meanFor { age and weight } // [`city`, `age`, `weight`] df.groupBy { city }.mean() // [`city`, `age`, `weight`, ...]     When statistics is applied to Pivot or PivotGroupBy , it is computed for every data group.  If statistic is applied in a mode that returns a single value for every data group, it will be stored in matrix cell without any name.      df.groupBy { city }.pivot { name.lastName }.mean { age } df.groupBy { city }.pivot { name.lastName }.meanOf { age / 2.0 }    val city by column<String?>() val age by column<Int>() val name by columnGroup() val lastName by name.column<String>() df.groupBy { city }.pivot { lastName }.mean { age } df.groupBy { city }.pivot { lastName }.meanOf { age() / 2.0 }    df.groupBy("city").pivot { "name"["lastName"] }.mean("age") df.groupBy("city").pivot { "name"["lastName"] }.meanOf { "age"<Int>() / 2.0 }     If statistic is applied in such a way that it returns separate value per every column in data group, every cell in matrix will contain DataRow with values for every aggregated column.    df.groupBy { city }.pivot { name.lastName }.meanFor { age and weight } df.groupBy { city }.pivot { name.lastName }.mean()    To group columns in aggregation results not by pivoted values, but by aggregated columns, apply separate flag:    df.groupBy { city }.pivot { name.lastName }.meanFor(separate = true) { age and weight } df.groupBy { city }.pivot { name.lastName }.mean(separate = true)
Counts the number of rows.    df.count()    Pass  to count number of rows that satisfy to that condition:    df.count { age > 15 }    When count is used in  or  aggregations, it counts rows for every data group:    df.groupBy { city }.count() df.pivot { city }.count { age > 18 } df.pivot { name.firstName }.groupBy { name.lastName }.count()
Computes the minimum / maximum of values.  Is available for Comparable columns. null and NaN values are ignored.    df.min() // min of values per every comparable column df.min { age and weight } // min of all values in `age` and `weight` df.minFor { age and weight } // min of values per `age` and `weight` separately df.minOf { (weight ?: 0) / age } // min of expression evaluated for every row df.minBy { age } // DataRow with minimal `age`      df.min() df.age.min() df.groupBy { city }.min() df.pivot { city }.min() df.pivot { city }.groupBy { name.lastName }.min()    See  for details on complex data aggregations.
Computes the sum of values.  null and NaN values are ignored.    df.sum() // sum of values per every numeric column df.sum { age and weight } // sum of all values in `age` and `weight` df.sumFor { age and weight } // sum of values per `age` and `weight` separately df.sumOf { (weight ?: 0) / age } // sum of expression evaluated for every row      df.age.sum() df.groupBy { city }.sum() df.pivot { city }.sum() df.pivot { city }.groupBy { name.lastName }.sum()    See  for details on complex data aggregations.
Computes the median of values.  Is available for Comparable columns. null and NaN values are ignored.    df.median() // median of values per every comparable column df.median { age and weight } // median of all values in `age` and `weight` df.medianFor { age and weight } // median of values per `age` and `weight` separately df.medianOf { (weight ?: 0) / age } // median of expression evaluated for every row      df.median() df.age.median() df.groupBy { city }.median() df.pivot { city }.median() df.pivot { city }.groupBy { name.lastName }.median()    See  for details on complex data aggregations.
Computes the mean of values.  Is available for numeric columns. Computed value has type Double . Use skipNA flag to skip null and NaN values.    df.mean() // mean of values per every numeric column df.mean(skipNA = true) { age and weight } // mean of all values in `age` and `weight`, skips NA df.meanFor(skipNA = true) { age and weight } // mean of values per `age` and `weight` separately, skips NA df.meanOf { (weight ?: 0) / age } // median of expression evaluated for every row      df.mean() df.age.mean() df.groupBy { city }.mean() df.pivot { city }.mean() df.pivot { city }.groupBy { name.lastName }.mean()    See  for details on complex data aggregations.
Computes the standard deviation of values.  Is available for numeric columns. Computed value has type Double .    df.std() // std of values per every numeric column df.std { age and weight } // std of all values in `age` and `weight` df.stdFor { age and weight } // std of values per `age` and `weight` separately, skips NA df.stdOf { (weight ?: 0) / age } // std of expression evaluated for every row      df.std() df.age.std() df.groupBy { city }.std() df.pivot { city }.std() df.pivot { city }.groupBy { name.lastName }.std()    See  for details on complex data aggregations.
Return DataFrame containing counts of unique values in DataFrame or DataColumn .  valueCounts(sort = true, ascending = false, dropNA = false) [ { columns } ]  Parameters:   sort: Boolean = true — sort by count  ascending: Boolean = false — sort in ascending order  dropNA: Boolean = true — don't include counts of NA value  columns = all — columns to use when counting unique combinations     df.city.valueCounts() df.valueCounts { name and city }
— cumulative sum (running total)
Computes cumulative sum of values in selected columns.  cumSum(skipNA = true) [ { columns } ]  Returns a DataFrame or DataColumn containing the cumulative sum.  Parameters:   skipNA — when true , ignores NA ( null or NaN ) values. When false , all values after first NA will be NaN (for Double and Float columns) or null (for integer columns).   Available for:        — cumulative sum per every data group     df.cumSum { weight } df.weight.cumSum() df.groupBy { city }.cumSum { weight }.concat()
— pairwise correlation of columns
Returns DataFrame with pairwise correlation between two sets of columns.  Computes Pearson correlation coefficient.  corr { columns1 } .with { columns2 } | .withItself()  To compute pairwise correlation between all columns in DataFrame use corr without arguments:  corr()  Available for numeric and Boolean columns. Boolean values are converted into 1 for true and 0 for false . All other columns are ignored.  If ColumnGroup is passed as target column for correlation, it will be unpacked into suitable nested columns.  Resulting DataFrame will have n1 rows and n2+1 columns, where n1 and n2 are numbers of columns in columns1 and columns2 correspondingly.  First column will have the name "column" and will contain names of columns in column1 . Other columns will have the same names as in columns2 and will contain computed correlation coefficients.  If exactly one ColumnGroup is passed in columns1 , first column in output will have its name.
— union of columns from several dataframes   — union of rows from several dataframes   — sql-like join of two dataframes by key columns
Returns DataFrame with union of columns from several given DataFrames .    df.add(df1, df2)    See  .
Returns DataFrame with the union of rows from several given DataFrames .    df.concat(df1, df2)      listOf(df1, df2).concat()    See  .
Joins two DataFrames by join columns.  join(otherDf, type = JoinType.Inner) [ { joinColumns } ] joinColumns: JoinDsl.(LeftDataFrame) -> Columns interface JoinDsl: LeftDataFrame { val right: RightDataFrame fun DataColumn.match(rightColumn: DataColumn) }  joinColumns is a  that defines column mapping for join:      df.join(other) { name match right.fullName }    val name by columnGroup() val fullName by columnGroup() df.join(other) { name match fullName }    df.join(other) { "name" match "fullName" }     If mapped columns have the same name, just select join columns from the left DataFrame :      df.join(other) { name and city }    val name by columnGroup() val city by column<String>() df.join(other) { name and city }    df.join(other, "name", "city")     If joinColumns is not specified, columns with the same name from both DataFrames will be used as join columns:    df.join(other)     Supported join types:   Inner (default) — only matched rows from left and right dataframes  Left — all rows from left dataframe, mismatches from right dataframe filled with null  Right — all rows from right dataframe, mismatches from left dataframe filled with null  Full — all rows from left and right dataframes, any mismatches filled with null  Exclude — only mismatched rows from left   For every join type there is a shortcut operation:      df.innerJoin(other) { name and city } df.leftJoin(other) { name and city } df.rightJoin(other) { name and city } df.fullJoin(other) { name and city } df.excludeJoin(other) { name and city }    val name by columnGroup() val city by column<String>() df.innerJoin(other) { name and city } df.leftJoin(other) { name and city } df.rightJoin(other) { name and city } df.fullJoin(other) { name and city } df.excludeJoin(other) { name and city }    df.innerJoin(other, "name", "city") df.leftJoin(other, "name", "city") df.rightJoin(other, "name", "city") df.fullJoin(other, "name", "city") df.excludeJoin(other, "name", "city")
// TODO
// TODO
// TODO
// TODO
// TODO
// TODO
DataFrame provides DSL for selecting arbitrary set of columns.  Column selectors are used in many operations:    df.select { age and name } df.fillNaNs { dfsOf<Double>() }.withZero() df.remove { cols { it.hasNulls() } } df.update { city }.notNull { it.lowercase() } df.gather { colsOf<Number>() }.into("key", "value") df.move { name.firstName and name.lastName }.after { city }    Select columns by name:      // by column name df.select { it.name } df.select { name } // by column path df.select { name.firstName } // with a new name df.select { name named "Full Name" } // converted df.select { name.firstName.map { it.lowercase() } } // column arithmetics df.select { 2021 - age } // two columns df.select { name and age } // range of columns df.select { name..age } // all children of ColumnGroup df.select { name.all() } // dfs traversal of all children columns df.select { name.allDfs() }    // by column name val name by columnGroup() df.select { it[name] } df.select { name } // by column path val firstName by name.column<String>() df.select { firstName } // with a new name df.select { name named "Full Name" } // converted df.select { firstName.map { it.lowercase() } } // column arithmetics val age by column<Int>() df.select { 2021 - age } // two columns df.select { name and age } // range of columns df.select { name..age } // all children of ColumnGroup df.select { name.all() } // dfs traversal of all children columns df.select { name.allDfs() }    // by column name df.select { it["name"] } // by column path df.select { it["name"]["firstName"] } df.select { "name"["firstName"] } // with a new name df.select { "name" named "Full Name" } // converted df.select { "name"["firstName"]<String>().map { it.uppercase() } } // column arithmetics df.select { 2021 - "age"<Int>() } // two columns df.select { "name" and "age" } // by range of names df.select { "name".."age" } // all children of ColumnGroup df.select { "name".all() } // dfs traversal of all children columns df.select { "name".allDfs() }     Select columns by column index:    // by index df.select { col(2) } // by several indices df.select { cols(0, 1, 3) } // by range of indices df.select { cols(1..4) }    Other column selectors:    // by condition df.select { cols { it.name().startsWith("year") } } df.select { startsWith("year") } // by type df.select { colsOf<String>() } // by type with condition df.select { colsOf<String?> { it.countDistinct() > 5 } } // all top-level columns df.select { all() } // first/last n columns df.select { take(2) } df.select { takeLast(2) } // all except first/last n columns df.select { drop(2) } df.select { dropLast(2) } // dfs traversal of all columns, excluding ColumnGroups from result df.select { allDfs() } // dfs traversal of all columns, including ColumnGroups in result df.select { allDfs(includeGroups = true) } // dfs traversal with condition df.select { dfs { it.name().contains(":") } } // dfs traversal of columns of given type df.select { dfsOf<String>() } // all columns except given column set df.select { except { colsOf<String>() } } // union of column sets df.select { take(2) and col(3) }    Modify the set of selected columns:    // first/last n columns in column set df.select { allDfs().take(3) } df.select { allDfs().takeLast(3) } // all except first/last n columns in column set df.select { allDfs().drop(3) } df.select { allDfs().dropLast(3) } // filter column set by condition df.select { allDfs().filter { it.name().startsWith("year") } } // exclude columns from column set df.select { allDfs().except { age } } // keep only unique columns df.select { (colsOf<Int>() and age).distinct() }
Kotlin DataFrame and Kotlin Collection represent two different approaches to data storage:   DataFrame stores data by fields/columns  Collection stores data by records/rows   Although DataFrame doesn't implement Collection or Iterable interface, it has many similar operations, such as  ,  ,  ,  ,  etc.  DataFrame has two-way compatibility with Map and List :   List<T> -> DataFrame<T> :   DataFrame<T> -> List<T> :   Map<String, List<*>> -> DataFrame<*> :   DataFrame<*> -> Map<String, List<*>> :    Columns, rows and values of DataFrame can be accessed as  ,  and  accordingly:    df.columns() // List<DataColumn> df.rows() // Iterable<DataRow> df.values() // Sequence<Any?>     DataFrame can be used as an intermediate object for transformation from one data structure to another.  Assume you have a list of instances of some  that you need to transform into some other format.    data class Input(val a: Int, val b: Int) val list = listOf(Input(1, 2), Input(3, 4))    You can convert this list into DataFrame using  extension:    val df = list.toDataFrame()    Mark original data class with  annotation to get  and perform data transformations.    @DataSchema data class Input(val a: Int, val b: Int) val df2 = df.add("c") { a + b }     To enable extension properties generation you should use  for Gradle or    After data is transformed, DataFrame can be exported into List of another data class using  or  extensions:    data class Output(val a: Int, val b: Int, val c: Int) val result = df2.toListOf<Output>()
Converts DataFrame into a List of data class instances by current DataFrame type argument.  toList()  Type of data class is defined by current type argument of DataFrame . If this type argument is not data class, exception will be thrown.  Data class properties are matched with DataFrame columns by name. If property type differs from column type  will be performed. If no automatic type conversion was found, exception will be thrown.  To export DataFrame into specific type of data class, use toListOf :   Converts DataFrame into a List of instances of given data class.    val df = dataFrameOf("name", "lastName", "age")("John", "Doe", 21) .group("name", "lastName").into("fullName") data class FullName(val name: String, val lastName: String) data class Person(val fullName: FullName, val age: Int) val persons = df.toListOf<Person>() // [Person(fullName = FullName(name = "John", lastName = "Doe"), age = 21)]
Converts DataFrame into Map<String, List<*>> from column names to column values.  toMap()
// TODO
// TODO
In the best scenario, your schema could be defined as simple as this:  dataframes { // output: build/generated/dataframe/main/kotlin/org/example/dataframe/JetbrainsRepositories.Generated.kt schema { data = "https://raw.githubusercontent.com/Kotlin/dataframe/master/data/jetbrains_repositories.csv" } }  Note than name of the file and the interface are normalized: split by '_' and ' ' and joined to camel case. You can set parsing options for CSV:  dataframes { // output: build/generated/dataframe/main/kotlin/org/example/dataframe/Securities.Generated.kt schema { data = "https://raw.githubusercontent.com/Kotlin/dataframe/1765966904c5920154a4a480aa1fcff23324f477/data/securities.csv" csvOptions { delimiter = ';' } } }  In this case output path will depend on your directory structure. For project with package org.example path will be build/generated/dataframe/main/kotlin/org/example/dataframe/Securities.Generated.kt . Note that name of the Kotlin file is derived from the name of the data file with the suffix .Generated and the package is derived from the directory structure with child directory dataframe . The name of the data schema itself is Securities . You could specify it explicitly:  schema { // output: build/generated/dataframe/main/kotlin/org/example/dataframe/MyName.Generated.kt data = "https://raw.githubusercontent.com/Kotlin/dataframe/master/data/jetbrains_repositories.csv" name = "MyName" }  If you want to change default package for all schemas:  dataframes { packageName = "org.example" // Schemas... }  Then you can set packageName for specific schema exclusively:  dataframes { // output: build/generated/dataframe/main/kotlin/org/example/data/OtherName.Generated.kt schema { packageName = "org.example.data" data = file("path/to/data.csv") } }  If you want non-default name and package, consider using fully-qualified name:  dataframes { // output: build/generated/dataframe/main/kotlin/org/example/data/OtherName.Generated.kt schema { name = "org.example.data.OtherName" data = file("path/to/data.csv") } }  By default, plugin will generate output in specified source set. Source set could be specified for all schemas or for specific schema:  dataframes { packageName = "org.example" sourceSet = "test" // output: build/generated/dataframe/test/kotlin/org/example/Data.Generated.kt schema { data = file("path/to/data.csv") } // output: build/generated/dataframe/integrationTest/kotlin/org/example/Data.Generated.kt schema { sourceSet = "integrationTest" data = file("path/to/data.csv") } }  But if you need generated files in other directory, set src :  dataframes { // output: schemas/org/example/test/OtherName.Generated.kt schema { data = "https://raw.githubusercontent.com/Kotlin/dataframe/master/data/jetbrains_repositories.csv" name = "org.example.test.OtherName" src = file("schemas") } }   Inside dataframes you can configure parameters that will apply to all schemas. Configuration inside schema will override these defaults for specific schema. Here is full DSL for declaring data schemas:  dataframes { sourceSet = "mySources" // [optional; default: "main"] packageName = "org.jetbrains.data" // [optional; default: common package under source set] visibility = // [optional; default: if explicitApiMode enabled then EXPLICIT_PUBLIC, else IMPLICIT_PUBLIC] // KOTLIN SCRIPT: DataSchemaVisibility.INTERNAL DataSchemaVisibility.IMPLICIT_PUBLIC, DataSchemaVisibility.EXPLICIT_PUBLIC // GROOVY SCRIPT: 'internal', 'implicit_public', 'explicit_public' withoutDefaultPath() // disable default path for all schemas // i.e. plugin won't copy "data" property of the schemas to generated companion objects // split property names by delimiters (arguments of this method), lowercase parts and join to camel case // enabled by default withNormalizationBy('_') // [optional: default: ['\t', '_', ' ']] withoutNormalization() // disable property names normalization schema { sourceSet /* String */ = "…" // [optional; override default] packageName /* String */ = "…" // [optional; override default] visibility /* DataSchemaVisibility */ = "…" // [optional; override default] src /* File */ = file("…") // [optional; default: file("build/generated/dataframe/$sourceSet/kotlin")] data /* URL | File | String */ = "…" // Data in JSON or CSV formats name = "org.jetbrains.data.Person" // [optional; default: from filename] csvOptions { delimiter /* Char */ = ';' // [optional; default: ','] } // See names normalization withNormalizationBy('_') // enable property names normalization for this schema and use these delimiters withoutNormalization() // disable property names normalization for this schema withoutDefaultPath() // disable default path for this schema withDefaultPath() // enable default path for this schema } }