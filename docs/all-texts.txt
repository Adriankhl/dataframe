Returns number of top-level columns in DataFrame .
Returns number of rows in DataFrame .
Returns number of distinct combinations of values in selected columns of DataFrame .      df.ndistinct { age and name }    val age by column<Int>() val name by columnGroup() df.ndistinct { age and name }    df.ndistinct("age", "name")     When columns are not specified, returns number of distinct rows in DataFrame .      df.ndistinct()
Returns list of names for top-level columns of DataFrame .
add - union of columns from several dataframes  concat - union of rows from several dataframes  join - sql-like join of two dataframes by key columns
Returns DataFrame with union of columns from several given DataFrames .    df.add(df1, df2)    See all use cases of 'add' operation .
Returns DataFrame with the union of rows from several given DataFrames .    df.concat(df1, df2)      listOf(df1, df2).concat()    See all use cases of 'concat' operation .
Joins two DataFrames by join columns.  join(otherDf, type = JoinType.Inner) [ { joinColumns } ] joinColumns: JoinDsl.(LeftDataFrame) -> Columns interface JoinDsl: LeftDataFrame { val right: RightDataFrame fun DataColumn.match(rightColumn: DataColumn) }  joinColumns is a column selector that defines column mapping for join:      df.join(other) { name match right.fullName }    val name by columnGroup() val fullName by columnGroup() df.join(other) { name match fullName }    df.join(other) { "name" match "fullName" }     If mapped columns have the same name, just select join columns from the left DataFrame :      df.join(other) { name and city }    val name by columnGroup() val city by column<String>() df.join(other) { name and city }    df.join(other, "name", "city")     If joinColumns is not specified, columns with the same name from both DataFrames will be used as join columns:    df.join(other)     Supported join types:   Inner (default) - only matched rows from left and right dataframes  Left - all rows from left dataframe, mismatches from right dataframe filled with null  Right - all rows from right dataframe, mismatches from left dataframe filled with null  Full - all rows from left and right dataframes, any mismatches filled with null  Exclude - only mismatched rows from left   For every join type there is a shortcut operation:      df.innerJoin(other) { name and city } df.leftJoin(other) { name and city } df.rightJoin(other) { name and city } df.fullJoin(other) { name and city } df.excludeJoin(other) { name and city }    val name by columnGroup() val city by column<String>() df.innerJoin(other) { name and city } df.leftJoin(other) { name and city } df.rightJoin(other) { name and city } df.fullJoin(other) { name and city } df.excludeJoin(other) { name and city }    df.innerJoin(other, "name", "city") df.leftJoin(other, "name", "city") df.rightJoin(other, "name", "city") df.fullJoin(other, "name", "city") df.excludeJoin(other, "name", "city")
Two ways to create DataFrame with a subset of columns:  indexing:      df[df.age, df.weight]    val age by column<Int>() val weight by column<Int?>() df[age, weight]    df["age", "weight"]     See DataFrame indexing  selecting:      df.select { age and weight }    val age by column<Int>() val weight by column<Int?>() df.select { age and weight } df.select(age, weight)    df.select { "age" and "weight" } df.select("age", "weight")     See column selectors
Start writing here.
Returns DataFrame with rows that satisfy row condition      df.filter { age > 18 && name.firstName.startsWith("A") }    val age by column<Int>() val name by columnGroup() val firstName by name.column<String>() df.filter { age() > 18 && firstName().startsWith("A") } // or df.filter { it[age] > 18 && it[firstName].startsWith("A") }    df.filter { "age"<Int>() > 18 && "name"["firstName"]<String>().startsWith("A") }      Returns DataFrame with rows that have value true in given column of type Boolean .      df.filterBy { isHappy }    val isHappy by column<Boolean>() df.filterBy { isHappy }    df.filterBy("isHappy")
Removes all rows that satisfy row condition      df.drop { weight == null || city == null }    val name by columnGroup() val weight by column<Int?>() val city by column<String?>() df.drop { weight() == null || city() == null } // or df.drop { it[weight] == null || it[city] == null }    df.drop { it["weight"] == null || it["city"] == null }      Remove rows with null values    df.dropNulls() // remove rows with null value in any column df.dropNulls(whereAllNull = true) // remove rows with null values in all columns df.dropNulls { city } // remove rows with null value in 'city' column df.dropNulls { city and weight } // remove rows with null value in 'city' OR 'weight' columns df.dropNulls(whereAllNull = true) { city and weight } // remove rows with null value in 'city' AND 'weight' columns     Remove rows with null , Double.NaN or Float.NaN values    df.dropNA() // remove rows containing null or Double.NaN in any column df.dropNA(whereAllNA = true) // remove rows with null or Double.NaN in all columns df.dropNA { weight } // remove rows where 'weight' is null or Double.NaN df.dropNA { age and weight } // remove rows where either 'age' or 'weight' is null or Double.NaN df.dropNA(whereAllNA = true) { age and weight } // remove rows where both 'age' and 'weight' are null or Double.NaN
Returns the first row that has the smallest value in the given column, or throws exception if DataFrame is empty.   Returns the first row that has the smallest value in the given column, or null if DataFrame is empty.
Returns the first row that has the largest value in the given column, or throws exception if DataFrame is empty.   Returns the first row that has the largest value in the given column, or null if DataFrame is empty.
Get single column by column name:      df.age df.name.lastName    val age by column<Int>() val name by columnGroup() val lastName by name.column<String>() df[age] df[lastName]    df["age"] df["name"]["firstName"]     Get single column by index (starting from 0):    df.getColumn(2) df.getColumnGroup(0).getColumn(1)
Return top-level columns of DataFrame as List<DataColumn<*>>
df.age[1] df[1].age    val age by column<String>() df[age][1] df[1][age]    df["age"][1] df[1]["age"]
Slicing means cutting a portion of DataFrame by continuous range of rows or columns.     by row indices (including boundaries):    df[1..2] df[0..2, 4..5]    See slice rows for other ways to select subset of rows.   by column indices (including boundaries):    df.select { cols(1..3) }    by column names:      df.select { age..weight }    val age by column<Int>() val weight by column<Int?>() df.select { age..weight }    df.select { "age".."weight" }     See Column Selectors for other ways to select subset of columns.
Iterate over rows:      for (row in df) { println(row.age) } df.forEachRow { println(it.age) } df.rows().forEach { println(it.age) }    val age by column<Int>() for (row in df) { println(row[age]) } df.forEachRow { println(it[age]) } df.rows().forEach { println(it[age]) }    for (row in df) { println(row["age"]) } df.forEachRow { println(it["age"]) } df.rows().forEach { println(it["age"]) }     Iterate over columns:    df.forEachColumn { println(it.name()) } df.columns().forEach { println(it.name()) }    Iterate over cells:    // from top to bottom, then from left to right df.values().forEach { println(it) } // from left to right, then from top to bottom df.values(byRow = true).forEach { println(it) }
Get single DataRow by index :    df[2]    Get single DataRow by row condition :      df.single { age == 45 } df.first { weight != null } df.minBy { age } df.maxBy { name.firstName.length } df.maxByOrNull { weight }    val age by column<Int>() val weight by column<Int?>() val name by columnGroup() val firstName by name.column<String>() df.single { age() == 45 } df.first { weight() != null } df.minBy(age) df.maxBy { firstName().length } df.maxByOrNull { weight() }    df.single { "age"<Int>() == 45 } df.first { it["weight"] != null } df.minBy("weight") df.maxBy { "name"["firstName"]<String>().length } df.maxByOrNull("weight")
Return rows of DataFrame as Iterable<DataRow> .   Return rows of DataFrame in reversed order.
Returns the single row that matches the given condition , or throws exception if there is no or more than one matching row.   Returns the single row that matches the given condition , or null if there is no or more than one matching row.
Returns the first row that matches the given condition , or throws exception if there is no matching rows.   Returns the first row that matches the given condition , or null if there is no matching rows.
Returns the last row that matches the given condition , or throws exception if there is no matching rows.   Returns the last row that matches the given condition , or null if there is no matching rows.
Start writing here.
Start writing here.
Start writing here.
Start writing here.
cumSum - cumulative sum (running total)
Computes cumulative sum of values in selected columns of DataFrame or in single DataColumn .  cumSum(skipNA = true) { columns }  Returns a DataFrame or DataColumn containing the cumulative sum.  Parameters:   skipNA: Boolean = true - ignore NA ( null or NaN ) values. When false , all values after first NA will be NaN (for Double and Float columns) or null (for integer columns).     df.cumSum { weight } df.weight.cumSum()
corr - pairwise correlation of columns
Returns DataFrame with pairwise correlation between two sets of columns.  Computes Pearson correlation coefficient.  corr { columns1 } .with { columns2 } | .withItself()  To compute pairwise correlation between all columns in DataFrame use corr without arguments:  corr()  Available for numeric and Boolean columns. Boolean values are converted into 1 for true and 0 for false . All other columns are ignored.  If ColumnGroup is passed as target column for correlation, it will be unpacked into suitable nested columns.  Resulting DataFrame will have n1 rows and n2+1 columns, where n1 and n2 are numbers of columns in columns1 and columns2 correspondingly.  First column will have the name "column" and will contain names of columns in column1 . Other columns will have the same names as in columns2 and will contain computed correlation coefficients.  If exactly one ColumnGroup is passed in columns1 , first column in output will have its name.
Kotlin DataFrame is a JVM Kotlin library for in-memory data manipulation.  Major highlights:   Readable, powerful and typesafe DSL for data wrangling  Reads and saves data in CSV and JSON formats  Supports hierarchical data schemas  Provides statically typed API for accessing data via code generation.  In Kotlin kernel for Jupyter DataFrame generates code on the fly  In Gradle projects, special plugin can assist you in generating schemas for your data     How to start using it? First, pick up the latest version of DataFrame here .   If you wish to play with data in interactive mode, setup Kotlin Kernel for Jupyter and run DataFrame there  If you have some JVM project, just add a dependency on DataFrame like it's described on Maven Central search site   Hope that this documentation will help you to implement all the things you want to do with your data. To get inspiration, take a look at examples folder. Puzzlers will quickly familiarize you with the power of DataFrame, and other notebooks and projects will show you some applications of DataFrame in practical data analysis.  If you found a bug, or have an idea for a new feature, please file an issue in DataFrame GitHub repository.  If you wish to contribute, you're welcome! Choose an issue you like, let us know that you're working on it, and prepare a pull request . We'll review and merge it if everything goes well.  You can also give us feedback or ask a question in #datascience channel of a Kotlin slack.  Good luck!
This documentation is written in such a way that it could be read sequentially and in this case, it provides all necessary information about the library, but at the same time the Operations section could be used as an API reference    Data frame is an abstraction for working with structured data. Essentially it’s a 2-dimensional table with labeled columns of potentially different types. You can think of it like a spreadsheet or SQL table, or a dictionary of series objects.  The handiness of this abstraction is not in the table itself but in a set of operations defined on it. Kotlin Dataframe library is an idiomatic Kotlin DSL defining such operations. The process of working with data frame is often called data wrangling which is the process of transforming and mapping data from one "raw" data form into another format that is more appropriate for analytics and visualization. The goal of data wrangling is to assure quality and useful data.     Hierarchical — Kotlin Dataframe is able to read and present data from different sources including not only plain CSV but also JSON . That’s why it has been designed hierarchical and allows nesting of columns and cells.    Interoperable — hierarchical data layout also opens a possibility of converting any objects structure in application memory to a data frame and vice versa.    Safe — Kotlin Dataframe provides a mechanism of on-the-fly generation of extension properties that correspond to the columns of frame. In interactive notebooks like Jupyter or Datalore, the generation runs after each cell execution. In IntelliJ IDEA there's a Gradle plugin for generation properties based on CSV and Json. Also, we’re working on a compiler plugin that infers and transforms data frame schema while typing.  The generated properties ensures you’ll never misspell column name and don’t mess up with its type, and of course nullability is also preserved.    Polymorphic — if all columns of dataframe are presented in some other dataframe, then the first one could be a superclass for latter. Thus, one can define a function on an interface with some set of columns and then execute it in a safe way on any dataframe which contains this set of columns.    Immutable — all operations on DataFrame produce new instance, while underlying data is reused everywhere it's possible     val df = DataFrame.read("titanic.csv") df.filter { survived && home.endsWith("NY") && age in 10..20 } .add("birthYear") { 1912 - age } .groupBy{ pclass }.aggregate{ // TODO }
By nature data frames are dynamic objects, column labels depend on input source and also new columns could be added or deleted while wrangling. Kotlin in contrast is a statically typed language and all types are defined and verified ahead of execution. That's why creating flexible, handy and at the same time, safe API to a data frame is a tricky thing.  In Kotlin Dataframe we provide four different ways to access data, and while they are essentially different, they look pretty similar in data wrangling DSL.   Here's a list of all API's in the order of increasing their safeness.    String API   Columns accessed by string representing their name. Type-checking is on runtime, name-checking is also on runtime.    Column Accessors API   Every column has a descriptor, a variable that representing its name and type.    Accessors API   Columns accessed by KProperty of some class. The name and type of column should match the name and type of property    Extension properties API Extension access properties are generating based on dataframe schema. Name and type of properties infers from name and type of corresponding columns.     Here's an example of how the same operations can be performed via different access APIs  In the most of the code snippets in this documentation there's a tab selector that allows switching across access APIs   TODO  TODO  TODO  TODO    String API is the simplest one and the most unsafe of all. The main advantage of it is that it can be used at any time, including accessing new columns in chain calls. So we can write something like:  df.add("weight") { ... } // adding a new column named `weight`, calculated by some expression .sortBy("weight") // sorting data frame rows by its value  So we don't need to interrupt a method chain and declare a column accessor or generate new properties.  In contrast, generated extension properties are the most convenient and safe API. Using it you can be always sure that you work with correct data and types. But its bottleneck — the moment of generation. To get new extension properties you have to run a cell in a notebook, which could lead to unnecessary variable declarations. Currently, we are working on compiler a plugin that generates these properties on the fly while user typing.  Column Accessors API is a kind of trade-off between safeness and ahead of the execution type declaration. It was designed to write code in IDE without notebook experience. It provides type-safe access to columns but doesn't ensure that the column presents in a particular data frame.  KProperty based API is useful when you have already declared classed in application business logic with fields that correspond columns of dataframe.    API  Type-checking  Column names checking  Column existence checking    Strings  Runtime  Runtime  Runtime    Column Accessors  Compile-time  Compile-time  Runtime    `KProperty` Accessors  Compile-time  Compile-time  Runtime    Extension Properties Accessors  Generation-time  Generation-time  Generation-time
String column names are the easiest way to access data in DataFrame:  val df = DataFrame.read("titanic.csv") df.filter { it["survived"] as Boolean }.groupBy("city").max("age")  or using invoke operator:  df.filter { "survived"<Boolean>() }.groupBy("city").max("age")  Note that if data frame doesn’t contain column with the string provided, or you try to cast to the wrong type it will lead to runtime exception.
Returns DataFrame with changed values in some cells. Column types can not be changed.  update { columns } [.where { rowCondition } ] [.at(rowIndices) ] .with { rowExpression } | .notNull { rowExpression } | .perCol { colExpression } | .perRowCol { rowColExpression } | .withValue(value) | .withNull() | .withZero() rowCondition: DataRow.(OldValue) -> Boolean rowExpression: DataRow.(OldValue) -> NewValue colExpression: DataColumn.(DataColumn) -> NewValue rowColExpression: DataRow.(DataColumn) -> NewValue  See column selectors and row expressions    df.update { age }.with { it * 2 } df.update { dfsOf<String>() }.with { it.uppercase() } df.update { weight }.at(1..4).notNull { it / 2 } df.update { name.lastName and age }.at(1, 3, 4).withNull()    Update with constant value:    df.update { city }.where { name.firstName == "Alice" }.withValue("Paris")    Update with value depending on row:    df.update { city }.with { name.firstName + " from " + it }    Update with value depending on column:    df.update { numberCols() }.perCol { mean(skipNA = true) }    Update with value depending on row and column:    df.update { stringCols() }.perRowCol { row, col -> col.name() + ": " + row.index() }
Replace missing values.   Replaces null values with given value or expression.    df.fillNulls { intCols() }.with { -1 } // same as df.update { intCols() }.where { it == null }.with { -1 }     Replaces Double.NaN and Float.NaN values with given value or expression.    df.fillNaNs { doubleCols() }.withZero()     Replaces null , Double.NaN and Float.NaN values with given value or expression.    df.fillNA { weight }.withValue(-1)
Returns DataFrame with changed values in some columns. Allows to change column types.  convert { columnsSelector } .with { rowExpression } | .perRowCol { rowColExpression } | .withValue(value) | to<Type>() | to { colExpression } rowExpression = DataRow.(OldValue) -> NewValue rowColExpression = DataRow.(DataColumn) -> NewValue colExpression = DataFrame.(DataColumn) -> DataColumn  See column selectors and row expressions    df.convert { age }.with { it.toDouble() } df.convert { dfsOf<String>() }.with { it.toCharArray().toList() }    convert supports automatic type conversions between the following types:   Int  String  Double  Long  Short  Float  BigDecimal  LocalDateTime  LocalDate  LocalTime     df.convert { age }.to<Double>() df.convert { numberCols() }.to<String>() df.convert { name.firstName and name.lastName }.to { it.length() } df.convert { weight }.toFloat()
Returns DataFrame in which given String columns are parsed into other types.  Special case of convert operation.    df.parse()    To parse only particular columns use column selector :    df.parse { age and weight }    parse tries to parse every String column into one of supported types in the following order:   Int  Long  LocalDateTime  LocalDate  LocalTime  URL  Double  Boolean  BigDecimal   Available parser options:   locale: Locale is used to parse numbers  dateTimePattern: String is used to parse date and time  dateTimeFormatter: DateTimeFormatter is used to parse date and time  nulls: List<String> is used to treat particular strings as null value. Default null strings: "null" and "NULL"     df.parse(options = ParserOptions(locale = Locale.CHINA, dateTimeFormatter = DateTimeFormatter.ISO_WEEK_DATE))    You can also set global parser options that will be used by default in read , convert and parse operations:    DataFrame.parser.locale = Locale.FRANCE DataFrame.parser.addDateTimePattern("dd.MM.uuuu HH:mm:ss")
Start writing here.
Start writing here.
Here is full DSL for declaring data schemas:  dataframes { sourceSet = "mySources" // [optional; default: "main"] packageName = "org.jetbrains.data" // [optional; default: common package under source set] visibility = // [optional; default: if explicitApiMode enabled then EXPLICIT_PUBLIC, else IMPLICIT_PUBLIC] // KOTLIN SCRIPT: DataSchemaVisibility.INTERNAL DataSchemaVisibility.IMPLICIT_PUBLIC, DataSchemaVisibility.EXPLICIT_PUBLIC // GROOVY SCRIPT: 'internal', 'implicit_public', 'explicit_public' schema { sourceSet /* String */ = "…" // [optional; override default] packageName /* String */ = "…" // [optional; override default] visibility /* DataSchemaVisibility */ = "…" // [optional; override default] src /* File */ = file("…") // [optional; default: file("src/$sourceSet/kotlin")] data /* URL | File | String */ = "…" // Data in JSON or CSV formats name = "org.jetbrains.data.Person" // [optional; default: from filename] } }   In the best scenario, your schema could be defined as simple as this:  dataframes { // output: src/main/kotlin/org/example/dataframe/Securities.Generated.kt schema { data = "https://raw.githubusercontent.com/Kotlin/dataframe/1765966904c5920154a4a480aa1fcff23324f477/data/securities.csv" } }  In this case output path will depend on your directory structure. For project with package org.example path will be src/main/kotlin/org/example/dataframe/Securities.Generated.kt . Note that name of the Kotlin file is derived from the name of the data file with the suffix .Generated and the package is derived from the directory structure with child directory dataframe . The name of the data schema itself is Securities . You could specify it explicitly:  schema { // output: src/main/kotlin/org/example/dataframe/MyName.Generated.kt data = "https://raw.githubusercontent.com/Kotlin/dataframe/1765966904c5920154a4a480aa1fcff23324f477/data/securities.csv" name = "MyName" }  If you want to change default package for all schemas:  dataframes { packageName = "org.example" // Schemas... }  Then you can set packageName for specific schema exclusively:  dataframes { // output: src/main/kotlin/org/example/data/OtherName.Generated.kt schema { packageName = "org.example.data" data = file("path/to/data.csv") } }  If you want non-default name and package, consider using fully-qualified name:  dataframes { // output: src/main/kotlin/org/example/data/OtherName.Generated.kt schema { name = org.example.data.OtherName data = file("path/to/data.csv") } }  By default, plugin will generate output in specified source set. Source set could be specified for all schemas or for specific schema:  dataframes { packageName = "org.example" sourceSet = "test" // output: src/test/kotlin/org/example/Data.kt schema { data = file("path/to/data.csv") } // output: src/integrationTest/kotlin/org/example/Data.kt schema { sourceSet = "integrationTest" data = file("path/to/data.csv") } }  But if you need generated files in other directory, set src :  dataframes { // output: schemas/org/example/test/OtherName.Generated.kt schema { data = "https://raw.githubusercontent.com/Kotlin/dataframe/1765966904c5920154a4a480aa1fcff23324f477/data/securities.csv" name = "org.example.test.OtherName" src = file("schemas") } }
Different ways to create dataframes from the data that already loaded into memory, are described in this section. You usually either create Column s from iterables , and then convert them into DataFrame , or create small dataframes for tests using vararg variants of the corresponding functions directly from values.  Creating dataframes from files and URLs is described in the next section
This section describes ways to create DataColumn .   Returns new column with given elements. Column type is deduced from compile-time type of elements, column name is taken from the name of the variable.    // Create ValueColumn with name 'student' and two elements of type String val student by columnOf("Alice", "Bob")    To assign column name explicitly, use named infix function and replace by with = .    val column = columnOf("Alice", "Bob") named "student"    When column elements are columns themselves, it returns ColumnGroup :    val firstName by columnOf("Alice", "Bob") val lastName by columnOf("Cooper", "Marley") // Create ColumnGroup with two nested columns val fullName by columnOf(firstName, lastName)    When column elements are DataFrames it returns FrameColumn :    val df1 = dataFrameOf("name", "age")("Alice", 20, "Bob", 25) val df2 = dataFrameOf("name", "temp")("Charlie", 36.6) // Create FrameColumn with two elements of type DataFrame val frames by columnOf(df1, df2)     Converts Iterable of values into column.    listOf("Alice", "Bob").toColumn("name")    To compute column type at runtime by scanning through actual values, set Infer.Type option.  To inspect values only for nullability set Infer.Nulls option.    val values: List<Any?> = listOf(1, 2.5) values.toColumn("data") // type: Any? values.toColumn("data", Infer.Type) // type: Number values.toColumn("data", Infer.Nulls) // type: Any     Converts Iterable of values into column of given type    val values: List<Any?> = listOf(1, 2.5) values.toColumnOf<Number?>("data") // type: Number?
This section describes ways to create DataFrame .   Returns DataFrame with given column names and values.    // DataFrame with 2 columns and 3 rows val df = dataFrameOf("name", "age")( "Alice", 15, "Bob", 20, "Charlie", 100 )      val name by columnOf("Alice", "Bob", "Charlie") val age by columnOf(15, 20, 22) // DataFrame with 2 columns val df = dataFrameOf(name, age)      val names = listOf("name", "age") val values = listOf( "Alice", 15, "Bob", 20, "Charlie", 22 ) val df = dataFrameOf(names, values)      // Multiplication table dataFrameOf(1..10) { x -> (1..10).map { x * it } }      // 5 columns filled with 7 random double values: val names = (1..5).map { "column$it" } dataFrameOf(names).randomDouble(7) // 5 columns filled with 7 random double values between 0 and 1 (inclusive) dataFrameOf(names).randomDouble(7, 0.0..1.0).print() // 5 columns filled with 7 random int values between 0 and 100 (inclusive) dataFrameOf(names).randomInt(7, 0..100).print()      val names = listOf("first", "second", "third") // DataFrame with 3 columns, fill each column with 15 `true` values val df = dataFrameOf(names).fill(15, true)     DataFrame from Iterable<DataColumn> :    val name by columnOf("Alice", "Bob", "Charlie") val age by columnOf(15, 20, 22) listOf(name, age).toDataFrame()    DataFrame from Map<String, List<*>> :    val map = mapOf("name" to listOf("Alice", "Bob", "Charlie"), "age" to listOf(15, 20, 22)) // DataFrame with 2 columns map.toDataFrame()     Creates DataFrame from Iterable of any objects .    data class Person(val name: String, val age: Int) val persons = listOf(Person("Alice", 15), Person("Bob", 20), Person("Charlie", 22)) val df = persons.createDataFrame()    Scans object properties using reflection and creates ValueColumn for every property. Scope of properties for scanning is defined at compile-time by formal types of objects in Iterable , so properties of implementation classes will not be scanned.  Specify depth parameter to perform deep object graph traversal and convert nested objects into ColumnGroups and FrameColumns :    data class Name(val firstName: String, val lastName: String) data class Score(val subject: String, val value: Int) data class Student(val name: Name, val age: Int, val scores: List<Score>) val students = listOf( Student(Name("Alice", "Cooper"), 15, listOf(Score("math", 4), Score("biology", 3))), Student(Name("Bob", "Marley"), 20, listOf(Score("music", 5))) ) val df = students.createDataFrame(depth = 2)    For detailed control over object graph transformation use configuration DSL. It allows you to exclude particular properties or classes from object graph traversal, compute additional columns and configure column grouping.    val df = students.createDataFrame { // add value column "year of birth" from { 2021 - it.age } // scan all properties properties(depth = 2) { exclude(Score::subject) // `subject` property will be skipped from object graph traversal preserve<Name>() // `Name` objects will be stored as-is without transformation into DataFrame } // add column group "summary" { "max score" from { it.scores.maxOf { it.value } } "min score" from { it.scores.minOf { it.value } } } }
Column accessors are created by property delegate  column . Column type should be passed as type argument, column name will be taken from the variable name.    val name by column<String>()    To assign column name explicitly, pass it as an argument.    val accessor by column<String>("complex column name")    You can also create column accessors for ColumnGroups and FrameColumns    val columns by columnGroup() val frames by frameColumn()     Deep column accessor references nested columns inside ColumnGroups .    val name by columnGroup() val firstName by name.column<String>()     Computed column accessor evaluates custom expression on every data access.      val fullName by df.column { name.firstName + " " + name.lastName } df[fullName]    val name by columnGroup() val firstName by name.column<String>() val lastName by name.column<String>() val fullName by column { firstName() + " " + lastName() } df[fullName]    val fullName by column { "name"["firstName"]<String>() + " " + "name"["lastName"]<String>() } df[fullName]     When expression depends only on one column, use map :    val age by column<Int>() val year by age.map { 2021 - it } df.filter { year > 2000 }
Removes duplicate rows. The rows in the resulting DataFrame are in the same order as they were in the original DataFrame .    df.distinct()    If columns are specified, resulting DataFrame will have only given columns with distinct values.      df.distinct { age and name } // same as df.select { age and name }.distinct()    val age by column<Int>() val name by columnGroup() df.distinct { age and name } // same as df.select { age and name }.distinct()    df.distinct("age", "name") // same as df.select("age", "name").distinct()      Keep only the first row for every group of rows grouped by some condition.      df.distinctBy { age and name } // same as df.groupBy { age and name }.mapToRows { group.first() }    val age by column<Int>() val name by columnGroup() val firstName by name.column<String>() df.distinctBy { age and name } // same as df.groupBy { age and name }.mapToRows { group.first() }    df.distinctBy("age", "name") // same as df.groupBy("age", "name").mapToRows { group.first() }
Start writing here.
Returns DataFrame sorted by one or several columns.  By default, columns are sorted in ascending order with null values going first. Available modifiers:   .desc - changes column sort order from ascending to descending  .nullsLast - forces null values to be placed at the end of the order       df.sortBy { age } df.sortBy { age and name.firstName.desc() } df.sortBy { weight.nullsLast() }    val age by column<Int>() val weight by column<Int?>() val name by columnGroup() val firstName by name.column<String>() df.sortBy { age } df.sortBy { age and firstName } df.sortBy { weight.nullsLast() }    df.sortBy("age") df.sortBy { "age" and "name"["firstName"].desc() } df.sortBy { "weight".nullsLast() }      Returns DataFrame sorted by one or several columns in descending order.      df.sortByDesc { age and weight }    val age by column<Int>() val weight by column<Int?>() df.sortByDesc { age and weight }    df.sortByDesc("age", "weight")      Returns DataFrame sorted with comparator.    df.sortWith { row1, row2 -> when { row1.age < row2.age -> -1 row1.age > row2.age -> 1 else -> row1.name.firstName.compareTo(row2.name.firstName) } }
Returns DataFrame with randomly reordered rows.    df.shuffle()
DataFrame represents a list of DataColumn .  Columns in dataframe must have equal size and unique names.  Learn how to:   Create dataframe  Read dataframe  Get an overview of dataframe  Access data in dataframe  Modify data in dataframe  Compute statistics for dataframe  Combine several dataframes
DataColumn represents a column of values. It can store objects of primitive or reference types, or other DataFrames .  See how to create columns    name: String - name of the column, should be unique within containing dataframe  path: ColumnPath - path to the column, depends on the way column was retrieved from dataframe  type: KType - type of elements in the column  hasNulls: Boolean - flag indicating whether column contains null values  values: Iterable<T> - column data  size: Int - number of elements in the column    DataColumn instances can be one of three subtypes: ValueColumn , ColumnGroup or FrameColumn   Represents a sequence of values.  It can store values of primitive (integers, strings, decimals etc.) or reference types. Currently, it uses List as underlying data storage.   Container for nested columns. Is used to create column hierarchy.   Special case of ValueColumn that stores other DataFrames as elements.  DataFrames stored in FrameColumn may have different schemas.  FrameColumn may appear after reading from JSON or other hierarchical data structures, or after grouping operations such as groupBy or pivot .    ColumnAccessors are used for typed data access in DataFrame :    val age by column<Int>() // Access fourth cell in the "age" column of dataframe `df`. // This expression returns `Int` because variable `age` has `ColumnAccessor<Int>` type. // If dataframe `df` has no column "age" or column "age" has type which is incompatible with `Int`, // runtime exception will be thrown. df[age][3] + 5 // Access first cell in the "age" column of dataframe `df`. df[0][age] * 2 // Returns new dataframe sorted by age column (ascending) df.sortBy(age) // Returns new dataframe with the column "year of birth" added df.add("year of birth") { 2021 - age } // Returns new dataframe containing only rows with age > 30 df.filter { age > 30 }    See how to create column accessor  ColumnAccessor stores column name (for top-level columns) or column path (for nested columns), has type argument that corresponds to column type , but doesn't contain any data. To convert ColumnAccessor into DataColumn just add values:    val age by column<Int>() val ageCol1 = age.withValues(15, 20) val ageCol2 = age.withValues(1..10)
DataRow represents a single record, one piece of data within a DataFrame    index(): Int - sequential row number in DataFrame , starts from 0  prev(): DataRow? - previous row ( null for the first row)  next(): DataRow? - next row ( null for the last row)  diff { rowExpression }: T - difference between results of row expression calculated for current and previous rows  values(): List<Any?> - list of all cell values from the current row  valuesOf<T>(): List<T> - list of values of given type  columnNames(): List<String> - list of all column names  namedValues(): List<NameValuePair<Any?>> - list of name-value pairs where name is a column name and value is cell value  namedValuesOf<T>(): List<NameValuePair<T>> - list of name-value pairs where value has given type  getRow(Int): DataRow - row from DataFrame by row index  near(Iterable<Int>): Sequence<DataRow> - sequence of the nearest rows by relative index: near(-1..1) will return previous, current and next row. Requested indices will be coerced to valid range and invalid indices will be skipped  rows(Iterable<Int>): Sequence<DataRow> - sequence of the rows by absolute index. Requested indices are not coerced to valid boundaries and you should care about it  get(column): T - cell value by this row and given column  df() - DataFrame that current row belongs to    Row expressions provide a value for every row of DataFrame and are used in add , filter , forEach , update and other operations.    // Row expression computes values for a new column df.add("fullName") { name.firstName + " " + name.lastName } // Row expression computes updated values df.update { weight }.at(1, 3, 4).with { prev()?.weight } // Row expression computes cell content for values of pivoted column df.pivot { city }.with { name.lastName.uppercase() }    Row expression signature: DataRow.(DataRow) -> T . Row values can be accessed with or without it keyword. Implicit and explicit argument represent the same DataRow object.   Row condition is a special case of row expression that returns Boolean .    // Row condition is used to filter rows by index df.filter { index() % 5 == 0 } // Row condition is used to drop rows where `age` is the same as in previous row df.drop { diff { age } == 0 } // Row condition is used to filter rows for value update df.update { weight }.where { index() > 4 && city != "Paris" }.withValue(50)    Row condition signature: DataRow.(DataRow) -> Boolean   The following statistics are available for DataRow :   rowMax  rowMin  rowSum  rowMean  rowStd   All these statistics are applied only to values of appropriate types and incompatible values will be ignored. For example, if DataFrame has columns of type String and Int , rowSum() will successfully compute sum of Int values in a row and ignore String values.
DataFrame transformation pipeline usually consists of several modification operations, such as filtering, sorting, grouping, pivoting, adding/removing columns etc. DataFrame API is designed in functional style so that the whole processing pipeline can be represented as a single statement with a sequential chain of operations. DataFrame object is immutable, so all operations will return a new DataFrame instance reusing underlying data structures as much as possible.    df.update { age }.where { city == "Paris" }.with { it - 5 } .filter { isHappy && age > 100 } .move { name.firstName and name.lastName }.after { isHappy } .merge { age and weight }.by { "Age: ${it[0]}, weight: ${it[1]}" }.into("info") .rename { isHappy }.into("isOK")     Simple operations (such as filter or select ) return DataFrame , but more complex operations return an intermediate object that is used for further configuration of the operation. Let's call such operations multiplex .  Every multiplex operation configuration consists of:   column selector that is used to select target columns for the operation  additional configuration  terminal function that returns modified DataFrame   Multiplex operations usually end with into or with function. The following naming convention is used:   into defines column names for storing operation results. Used in move , group , split , merge , gather , groupBy , rename .  with defines row-wise data transformation using row expression . Used in update , convert , replace , pivot .     add - add columns  append - add rows  columns - get list of columns  concat - union rows  convert - change column values and/or column types  corr - pairwise correlation of columns  cumSum - cumulative sum of column values  describe - basic column statistics  distinct / distinctBy - remove duplicated rows  drop / dropLast / dropNulls / dropNA - remove rows be condition  explode - spread lists and dataframes vertically into new rows  fillNulls / fillNaNs / fillNA - replace missing values  filter / filterBy - filter rows  first / firstOrNull - first row by condition  flatten - remove column groupings recursively  forEachRow / forEachColumn - iterate over rows or columns  format - conditional formatting for cell rendering  gather - convert columns into key-value pairs  getColumn / getColumnOrNull / getColumnGroup / getColumns - get one or several columns  group - group columns into ColumnGroup  groupBy - group rows by key columns  head - top 5 rows  implode - collapse column values into lists  insert - insert column  join - join dataframes by key columns  last / lastOrNull - last row by condition  map - map DataFrame columns to a new DataFrame or DataColumn  max / maxBy / maxOf / maxFor - max of values  mean / meanOf / meanFor - average of values  median / medianOf / medianFor - median of values  merge - merge several columns into one  min / minBy / minOf / minFor - min of values  move - move columns or change column groupings  ncol - number of columns  ndistinct - number of unique rows  nrow - number of rows  parse - convert String values into appropriate types  pivot / pivotCounts / pivotMatches - convert column values into new columns  remove - remove columns  rename - rename columns  replace - replace columns  rows / rowsReversed  schema - schema of column hierarchy  select - select subset of columns  shuffled - reorder rows randomly  single / singleOrNull - single row by condition  sortBy / sortByDesc / sortWith - sort rows  split - split column values into several columns or new rows  std / stdOf / stdFor - standard deviation of values  sum / sumOf / sumFor - sum of values  take / takeLast - first/last rows  ungroup - remove column grouping  update - change column values preserving column types  valueCounts - counts for unique values  xs - filter by key values
Returns DataFrame containing first n (default 5) rows.    df.head(3)    Similar to take .
Returns DataFrameSchema object with DataFrame schema description. It can be printed to see column structure.  ColumnGroups are marked by indentation:    df.schema()    Output:  name: firstName: String lastName: String age: Int city: String? weight: Int? isHappy: Boolean  FrameColumns are marked with * :    df.groupBy { city }.schema()    Output:  city: String? group: * name: firstName: String lastName: String age: Int city: String? weight: Int? isHappy: Boolean
Returns DataFrame with general statistics for all ValueColumns .  ColumnGroups and FrameColumns are traversed recursively down to ValueColumns .  Collected statistics:   name - column name  path - path to the column (for hierarchical DataFrame )  type - type of values  count - number of rows  unique - number of unique values  nulls - number of null values  top - the most common not null value  freq - top value frequency  mean - mean value (for numeric columns)  std - standard deviation (for numeric columns)  min - minimal value (for comparable columns)  median - median value (for comparable columns)  max - maximum value (for comparable columns)     df.describe()    To describe only specific columns, pass them as an argument:      df.describe { age and name.all() }    val age by column<Int>() val name by columnGroup() df.describe { age and name.all() }    df.describe { "age" and "name".all() }
Get rows or columns :    df.columns() // List<DataColumn> df.rows() // Iterable<DataRow> df.values() // Sequence<Any?>    Learn how to:   Access data by index  Slice portion of data  Iterate over data  Get single row  Get single column
Start writing here.
Start writing here.
DataFrame provides DSL for selecting arbitrary set of columns.  Column selectors are used in many operations:    df.select { age and name } df.fillNaNs { dfsOf<Double>() }.withZero() df.remove { cols { it.hasNulls() } } df.update { city }.notNull { it.lowercase() } df.gather { numberCols() }.into("key", "value") df.move { name.firstName and name.lastName }.after { city }    Select columns by name:      // by column name df.select { it.name } df.select { name } // by column path df.select { name.firstName } // with a new name df.select { name named "Full Name" } // converted df.select { name.firstName.map { it.lowercase() } } // column arithmetics df.select { 2021 - age } // two columns df.select { name and age } // range of columns df.select { name..age } // all children of ColumnGroup df.select { name.all() } // dfs traversal of children columns df.select { name.dfs() }    // by column name val name by columnGroup() df.select { it[name] } df.select { name } // by column path val firstName by name.column<String>() df.select { firstName } // with a new name df.select { name named "First Name" } // converted df.select { firstName.map { it.lowercase() } } // column arithmetics val age by column<Int>() df.select { 2021 - age } // two columns df.select { name and age } // range of columns df.select { name..age } // all children of ColumnGroup df.select { name.all() } // dfs traversal of children columns df.select { name.dfs() }    // by column name df.select { it["name"] } // by column path df.select { it["name"]["firstName"] } df.select { "name"["firstName"] } // with a new name df.select { "name" named "First Name" } // converted df.select { "name"["firstName"]<String>().map { it.uppercase() } } // column arithmetics df.select { 2021 - "age"() } // two columns df.select { "name" and "age" } // by range of names df.select { "name".."age" } // all children of ColumnGroup df.select { "name".all() } // dfs traversal of children columns df.select { "name".dfs() }     Select columns by column index:    // by index df.select { col(2) } // by several indices df.select { cols(0, 1, 3) } // by range of indices df.select { cols(1..4) }    Other column selectors:    // by condition df.select { cols { it.name.startsWith("year") } } // by type df.select { colsOf<String>() } df.select { stringCols() } // by type with condition df.select { colsOf<String> { !it.hasNulls() } } df.select { stringCols { !it.hasNulls() } } // all top-level columns df.select { all() } // first/last n columns df.select { take(2) } df.select { takeLast(2) } // all except first/last n columns df.select { drop(2) } df.select { dropLast(2) } // dfs traversal of columns df.select { dfs() } // dfs traversal with condition df.select { dfs { it.name.contains(":") } } // columns of given type in dfs traversal df.select { dfsOf<String>() } // all columns except given column set df.select { except { colsOf<String>() } } // union of column sets df.select { take(2) and col(3) }    Modify the set of selected columns:    // first/last n columns in column set df.select { dfs().take(3) } df.select { dfs().takeLast(3) } // all except first/last n columns in column set df.select { dfs().drop(3) } df.select { dfs().dropLast(3) } // filter column set by condition df.select { dfs().filter { it.name.startsWith("year") } } // exclude columns from column set df.select { dfs().except { age } }
DataFrame doesn't implement Iterable interface, but redefines some of extension functions available for Iterable :    df.forEachRow { println(it) } df.take(5) df.drop(2) df.chunked(10)    To convert DataFrame into Iterable / Sequence of rows, columns or cell values use the following functions:    df.columns() // List<DataColumn> df.rows() // Iterable<DataRow> df.values() // Sequence<Any?>
Returns DataFrame which contains all columns from original DataFrame followed by newly added columns. Original DataFrame is not modified.  Create new column and add it to DataFrame :  add(columnName) { rowExpression } rowExpression: DataRow.(DataRow) -> Value      df.add("year of birth") { 2021 - age }    val age by column<Int>() val yearOfBirth by column<Int>("year of birth") df.add(yearOfBirth) { 2021 - age }    df.add("year of birth") { 2021 - "age"<Int>() }     See row expressions  Create and add several columns to DataFrame :  add { columnMapping columnMapping ... } columnMapping = column into columnName | columnName from column | columnName from { rowExpression }      df.add { "year of birth" from 2021 - age age gt 18 into "is adult" name.lastName.length() into "last name length" "full name" from { name.firstName + " " + name.lastName } }    val yob = column<Int>("year of birth") val lastNameLength = column<Int>("last name length") val age by column<Int>() val isAdult = column<Boolean>("is adult") val fullName = column<String>("full name") val name by columnGroup() val firstName by name.column<String>() val lastName by name.column<String>() df.add { yob from 2021 - age age gt 18 into isAdult lastName.length() into lastNameLength fullName from { firstName() + " " + lastName() } }    df.add { "year of birth" from 2021 - "age"<Int>() "age"<Int>() gt 18 into "is adult" "name"["lastName"]<String>().length() into "last name length" "full name" from { "name"["firstName"]<String>() + " " + "name"["lastName"]<String>() } }     Add existing column to DataFrame :    val score by columnOf(4, 3, 5, 2, 1, 3, 5) df.add(score) df + score    Add all columns from another DataFrame :    df.add(df1, df2)
Creates DataFrame or DataColumn with values computed from rows of original DataFrame .  Map into DataColumn :  map(columnName) { rowExpression }: DataColumn rowExpression: DataRow.(DataRow) -> Value  See row expressions  Map into DataFrame :  map { columnMapping columnMapping ... } : DataFrame columnMapping = column into columnName | columnName from column | columnName from { rowExpression } | +column      df.map { "year of birth" from 2021 - age age gt 18 into "is adult" name.lastName.length() into "last name length" "full name" from { name.firstName + " " + name.lastName } +city }    val yob = column<Int>("year of birth") val lastNameLength = column<Int>("last name length") val age by column<Int>() val isAdult = column<Boolean>("is adult") val fullName = column<String>("full name") val name by columnGroup() val firstName by name.column<String>() val lastName by name.column<String>() val city by column<String?>() df.map { yob from 2021 - age age gt 18 into isAdult lastName.length() into lastNameLength fullName from { firstName() + " " + lastName() } +city }    df.map { "year of birth" from 2021 - "age"<Int>() "age"<Int>() gt 18 into "is adult" "name"["lastName"]<String>().length() into "last name length" "full name" from { "name"["firstName"]<String>() + " " + "name"["lastName"]<String>() } +"city" }
Returns DataFrame without selected columns.  remove { columns }  See Column Selectors      df.remove { name and weight }    val name by columnGroup() val weight by column<Int?>() df.remove { name and weight }    df.remove("name", "weight")
move - moves given columns or changes column grouping in DataFrame  rename - renames given columns in DataFrame
pivot - transforms column values into new columns (long to wide)  gather - collects values from several columns into two key and value columns (wide to long)
Splits the rows of DataFrame and groups them horizontally into new columns based on values from one or several columns of original DataFrame .  Pass a column to pivot function to use its values as grouping keys and names for new columns.      df.pivot { city }    val city by column<String?>() df.pivot { city }    df.pivot("city")     Returns Pivot : an intermediate object that can be configured for further transformation and aggregation of data.  See pivot aggregations  By default, pivoted column will be replaced with new columns generated from its values. Instead, you can nest new columns as sub-columns of original column using inward flag:      df.pivot(inward = true) { city }    val city by column<String?>() df.pivot(inward = true) { city }    df.pivot("city", inward = true)     To pivot several columns in one operation you can combine them using and or then infix function:   and will pivot columns independently  then will create column hierarchy based on possible combinations of column values       df.pivot { city and name.firstName } df.pivot { city then name.firstName }    val city by column<String?>() val name by columnGroup() val firstName by name.column<String>() df.pivot { city and firstName } df.pivot { city then firstName }    df.pivot { "city" and "name"["firstName"] } df.pivot { "city" then "name"["firstName"] }      To create matrix table that is expanded both horizontally and vertically, apply groupBy function at Pivot passing the columns for vertical grouping. Reversed order of pivot and groupBy operations will produce the same result.      df.pivot { city }.groupBy { name } // same as df.groupBy { name }.pivot { city }    val city by column<String?>() val name by columnGroup() df.pivot { city }.groupBy { name } // same as df.groupBy { name }.pivot { city }    df.pivot("city").groupBy("name") // same as df.groupBy("name").pivot("city")     Combination of pivot and groupBy operations returns PivotGroupBy that can be used for further aggregation of data groups within matrix cells.  See pivot aggregations  To group by all columns except pivoted use groupByOther :    df.pivot { city }.groupByOther()    Pivot operation can be performed without any data aggregation:   Pivot object can be converted to DataRow or DataFrame .  GroupedPivot object can be converted to DataFrame .   Generated columns will have type FrameColumn and will contain data groups.    df.pivot { city }.toDataRow() df.pivot { city }.groupBy { name }.toDataFrame()     To aggregate data groups in Pivot or PivotGroupBy with one or several statistics use aggregate :      df.pivot { city }.aggregate { minBy { age }.name } df.pivot { city }.groupBy { name.firstName }.aggregate { meanFor { age and weight } into "means" stdFor { age and weight } into "stds" maxByOrNull { weight }?.name?.lastName into "biggest" }    val city by column<String?>() val name by columnGroup() val firstName by name.column<String>() val age by column<Int>() val weight by column<Int?>() df.pivot { city }.aggregate { minBy(age).name } df.pivot { city }.groupBy { firstName }.aggregate { meanFor { age and weight } into "means" stdFor { age and weight } into "stds" maxByOrNull(weight)?.name?.lastName into "biggest" }    df.pivot("city").aggregate { minBy("age")["name"] } df.pivot("city").groupBy { "name"["firstName"] }.aggregate { meanFor("age", "weight") into "means" stdFor("age", "weight") into "stds" maxByOrNull("weight")?.getColumnGroup("name")?.get("lastName") into "biggest" }     Shortcuts for common aggregation functions are also available:      df.pivot { city }.maxFor { age and weight } df.groupBy { name }.pivot { city }.median { age }    val city by column<String?>() val name by columnGroup() val age by column<Int>() val weight by column<Int?>() df.pivot { city }.maxFor { age and weight } df.groupBy { name }.pivot { city }.median { age }    df.pivot("city").maxFor("age", "weight") df.groupBy("name").pivot("city").median("age")     By default, when aggregation function produces several values for data group, column hierarchy in resulting DataFrame will be indexed first by pivot keys and then by the names of aggregated values. To reverse this order so that resulting columns will be indexed first by names of aggregated values and then by pivot keys, use separate=true flag that is available in multi-result aggregation operations, such as aggregate or <stat>For :      df.pivot { city }.maxFor(separate = true) { age and weight } df.pivot { city }.aggregate(separate = true) { min { age } into "min age" maxOrNull { weight } into "max weight" }    val city by column<String?>() val age by column<Int>() val weight by column<Int?>() df.pivot { city }.maxFor(separate = true) { age and weight } df.pivot { city }.aggregate(separate = true) { min { age } into "min age" maxOrNull { weight } into "max weight" }    df.pivot("city").maxFor("age", "weight", separate = true) df.pivot("city").aggregate(separate = true) { min("age") into "min age" maxOrNull("weight") into "max weight" }     By default, any aggregation function will result in null value for those matrix cells, where intersection of column and row keys produced an empty data group. You can specify default value for any aggregation by default infix function. This value will replace all null results of aggregation function over non-empty data groups as well. To use one default value for all aggregation functions, use default() before aggregation.      df.pivot { city }.groupBy { name }.aggregate { min { age } default 0 } df.pivot { city }.groupBy { name }.aggregate { median { age } into "median age" default 0 minOrNull { weight } into "min weight" default 100 } df.pivot { city }.groupBy { name }.default(0).min()    val city by column<String?>() val age by column<Int>() val weight by column<Int?>() val name by columnGroup() df.pivot { city }.groupBy { name }.aggregate { min { age } default 0 } df.pivot { city }.groupBy { name }.aggregate { median { age } into "median age" default 0 minOrNull { weight } into "min weight" default 100 } df.pivot { city }.groupBy { name }.default(0).min()    df.pivot("city").groupBy("name").aggregate { min("age") default 0 } df.pivot("city").groupBy("name").aggregate { median("age") into "median age" default 0 minOrNull("weight") into "min weight" default 100 } df.pivot("city").groupBy("name").default(0).min()      pivot operation can also be used inside aggregate body of GroupBy . This allows to combine column pivoting with other aggregation functions:      df.groupBy { name.firstName }.aggregate { pivot { city }.aggregate(separate = true) { mean { age } into "mean age" count() into "count" } count() into "total" }    val city by column<String?>() val name by columnGroup() val firstName by name.column<String>() val age by column<Int>() df.groupBy { firstName }.aggregate { pivot { city }.aggregate(separate = true) { mean { age } into "mean age" count() into "count" } count() into "total" }    df.groupBy { "name"["firstName"] }.aggregate { pivot("city").aggregate(separate = true) { mean("age") into "mean age" count() into "count" } count() into "total" }      Pivots with count statistics one or several columns preserving all other columns of DataFrame or GroupBy .    df.pivotCounts { city } // same as df.pivot { city }.groupByOther().count() df.groupBy { name }.pivotCounts { city } // same as df.groupBy { name }.pivot { city }.count() // same as df.groupBy { name }.aggregate { pivotCounts { city } }     Pivots with Boolean statistics one or several columns preserving all other columns of DataFrame .    df.pivotMatches { city } // same as df.pivot { city }.groupByOther().matches() df.groupBy { name }.pivotMatches { city } // same as df.groupBy { name }.pivot { city }.matches() // same as df.groupBy { name }.aggregate { pivotMatches { city } }
Converts several columns into two columns key and value . key column will contain names of original columns, value column will contain values from original columns.  This operation is reverse to pivot  gather { columns } [.explodeLists()] [.cast<Type>()] [.notNull()] [.where { valueFilter }] [.mapKeys { keyTransform }] [.mapValues { valueTransform }] .into(keyColumn, valueColumn) | .keysInto(keyColumn) | .valuesInto(valueColumn) valueFilter: (value) -> Boolean keyTransform: (columnName: String) -> K valueTransform: (value) -> R  See column selectors  Configuration options:   explodeLists - gathered values of type List will be exploded into their elements, so where , cast , notNull and mapValues will be applied to list elements instead of lists themselves  cast - inform compiler about the expected type of gathered elements. This type will be passed to where and mapKeys lambdas  notNull - skip gathered null values  where - filter gathered values  mapKeys - transform gathered column names (keys)  mapValues - transform gathered column values   Storage options:   into(keyColumn, valueColumn) - store gathered key-value pairs in two new columns with names keyColumn and valueColumn  keysInto(keyColumn) - store only gathered keys (column names) in a new column keyColumn  valuesInto(valueColumn) - store only gathered values in a new column valueColumn     pivoted.gather { "London".."Tokyo" }.into("city", "population")      pivoted.gather { "London".."Tokyo" } .cast<Int>() .where { it > 10 } .mapKeys { it.lowercase() } .mapValues { 1.0 / it } .into("city", "density")
Adds one or several rows to DataFrame  df.append ( "Mike", 15, "John", 17, "Bill", 30)
Computes the median of values.  Is available for Comparable columns. null and NaN values are ignored.    df.median() // median of values per every comparable column df.median { age and weight } // median of all values in `age` and `weight` df.medianFor { age and weight } // median of values per `age` and `weight` separately df.medianOf { (weight ?: 0) / age } // median of expression evaluated for every row      df.median() df.age.median() df.groupBy { city }.median() df.pivot { city }.median() df.pivot { city }.groupBy { name.lastName }.median()    See statistics for details on complex data aggregations.
Computes the mean of values.  Is available for numeric columns. Computed value has type Double . Use skipNA flag to skip null and NaN values.    df.mean() // mean of values per every numeric column df.mean(skipNA = true) { age and weight } // mean of all values in `age` and `weight`, skips NA df.meanFor(skipNA = true) { age and weight } // mean of values per `age` and `weight` separately, skips NA df.meanOf { (weight ?: 0) / age } // median of expression evaluated for every row      df.mean() df.age.mean() df.groupBy { city }.mean() df.pivot { city }.mean() df.pivot { city }.groupBy { name.lastName }.mean()    See statistics for details on complex data aggregations.
Computes the standard deviation of values.  Is available for numeric columns. Computed value has type Double .    df.std() // std of values per every numeric column df.std { age and weight } // std of all values in `age` and `weight` df.stdFor { age and weight } // std of values per `age` and `weight` separately, skips NA df.stdOf { (weight ?: 0) / age } // std of expression evaluated for every row      df.std() df.age.std() df.groupBy { city }.std() df.pivot { city }.std() df.pivot { city }.groupBy { name.lastName }.std()    See statistics for details on complex data aggregations.
Return DataFrame containing counts of unique values in DataFrame or DataColumn .  valueCounts(sort = true, ascending = false, dropNA = false) [ { columns } ]  Parameters:   sort: Boolean = true - sort by count  ascending: Boolean = false - sort in ascending order  dropNA: Boolean = true - don't include counts of NA value  columns = all - columns to use when counting unique combinations     df.city.valueCounts() df.valueCounts { name and city }
Working with data, you have to read it - from disk or from remote URLs - and write it on disk. This section describes how to do it. For now, only CSV, TSV and JSON formats are supported.
DataFrame supports CSV and JSON input formats.  read method automatically detects input format based on file extension and content  DataFrame.read("input.csv")  Input string can be a file path or URL.   All these calls are valid:  import java.io.File import java.net.URL DataFrame.readCSV("input.csv") DataFrame.readCSV(File("input.csv")) DataFrame.readCSV(URL("https://raw.githubusercontent.com/Kotlin/dataframe/master/data/securities.csv"))  All readCSV overloads support different options. For example, you can specify custom delimiter if it differs from , , charset and headers names if your CSV is missing them    val df = DataFrame.readCSV( file, delimiter = '|', headers = listOf("A", "B", "C", "D"), parserOptions = ParserOptions(nulls = setOf("not assigned")) )    Column types will be inferred from the actual CSV data. Suppose that CSV from the previous example had the following content:   A B C D  12 tuv 0.12 true  41 xyz 3.6 not assigned  89 abc 7.1 false   Dataframe schema we get is:  A: Int B: String C: Double D: Boolean?   Basics for reading JSONs are the same: you can read from file or from remote URL.  DataFrame.readJson("https://covid.ourworldindata.org/data/owid-covid-data.json")  Note that after reading a JSON with a complex structure, you can get hierarchical dataframe: dataframe with GroupColumn s and FrameColumn s.  Also note that type inferring process for JSON is much simpler than for CSV. JSON string literals are always supposed to have String type, number literals take different Number kinds, boolean literals are converted to Boolean .  Let's take a look at the following JSON:  [ { "A": "1", "B": 1, "C": 1.0, "D": true }, { "A": "2", "B": 2, "C": 1.1, "D": null }, { "A": "3", "B": 3, "C": 1, "D": false }, { "A": "4", "B": 4, "C": 1.3, "D": true } ]  We can read it from file    val df = DataFrame.readJson(file)    Corresponding dataframe schema will be  A: String B: Int C: Number D: Boolean?  Column A has String type because all values are string literals, no implicit conversion is performed. Column C has Number type because it's the least common type for Int and Double .
DataFrames can be saved in CSV or JSON formats.   You can write your dataframe in CSV format to file, to string or to Appendable (i.e. to Writer ).    df.writeCSV(file)      val csvStr = df.writeCSVStr(CSVFormat.DEFAULT.withDelimiter(';').withRecordSeparator(System.lineSeparator()))     You can write your dataframe in JSON format to file, to string or to Appendable (i.e. to Writer ).    df.writeJson(file)      val jsonStr = df.writeJsonStr(prettyPrint = true)
General information about DataFrame :   nrow() - number of rows  ncol() - number of columns  ndistinct() - number of distinct columns  columnNames() - list of column names  head(n) - first n rows (default 5)  schema() - schema of columns  describe() - general statistics for every column
For frequently accessed columns type casting can be reduced by Column Accessors:  val survived by column<Boolean>() // accessor for Boolean column with name 'survived' val home by column<String>() val age by column<Int?>()  Now columns can be accessed in a type-safe way:  df.filter { it[survived] && it[home].endsWith("NY") && it[age] in 10..20 }  or just using invoke operator at column accessors:  df.filter { survived() && home().endsWith("NY") && age() in 10..20 }  Note that it still doesn’t solve the problem of whether the column actually exists in a data frame, but type-safety is now preserved.
// TODO
When DataFrame is used within Jupyter Notebooks or Datalore with Kotlin Kernel, after every cell execution all new global variables of type DataFrame are analyzed and replaced with typed DataFrame wrapper with auto-generated extension properties for data access:  val df = DataFrame.read("titanic.csv")  Now data can be accessed by . member accessor  df.filter { it.survived && it.home.endsWith("NY") && it.age in 10..20 } //it can be omitted df.filter { survived && home.endsWith("NY") && age in 10..20 }  Extension properties are generated for DataSchema that is extracted from DataFrame instance after REPL line execution. After that DataFrame variable is typed with its own DataSchema , so only valid extension properties corresponding to actual columns in DataFrame will be allowed by the compiler and suggested by completion.  Also, extension properties can be generated in IntelliJ IDEA based on csv or json files using Kotlin Dataframe Gradle plugin.  // TODO: Link to plugin here  In notebooks generated properties won't appear and be updated until the cell has been executed. It often means that you have to introduce new variable frequently to sync extension properties with actual schema
DataFrame can represent hierarchical data structures using two special types of columns:   ColumnGroup is a group of columns  FrameColumn is a column of dataframes   You can create DataFrame  from json or from graph of Kotlin objects preserving original tree structure.  Hierarchical columns can also appear as a result of some modification operations :   group produces ColumnGroup  groupBy produces FrameColumn  pivot may produce FrameColumn  split may produce ColumnGroup  implode converts ColumnGroup into FrameColumn  explode converts FrameColumn into ColumnGroup  merge converts ColumnGroups into FrameColumn
count  sum  min/max  mean  median  std  valueCounts   Every summary statistics can be used in aggregations of:   DataFrame  DataColumn  GroupBy  Pivot  PivotGroupBy     df.mean() df.age.sum() df.groupBy { city }.mean() df.pivot { city }.median() df.pivot { city }.groupBy { name.lastName }.std()    sum , mean , std are available for numeric columns of types Int , Double , Float , BigDecimal , Long , Byte .  min/max , median are available for Comparable columns.  When statistics x is applied to several columns, it can be computed in several modes:   x(): DataRow computes separate value per every suitable column  x { columns }: Value computes single value across all given columns  xFor { columns }: DataRow computes separate value per every given column  xOf { rowExpression }: Value computes single value across results of row expression evaluated for every row   min and max statistics have additional mode by :   minBy { rowExpression }: DataRow finds a row with minimal result of expression     df.sum() // sum of values per every numeric column df.sum { age and weight } // sum of all values in `age` and `weight` df.sumFor { age and weight } // sum of values per `age` and `weight` separately df.sumOf { (weight ?: 0) / age } // sum of expression evaluated for every row     When statistics is applied to GroupBy , it is computed for every data group.  If statistic is applied in a mode that returns a single value for every data group, it will be stored in a single column named by statistic name.    df.groupBy { city }.mean { age } // [`city`, `mean`] df.groupBy { city }.meanOf { age / 2 } // [`city`, `mean`]    You can also pass custom name for aggregated column:    df.groupBy { city }.mean("mean age") { age } // [`city`, `mean age`] df.groupBy { city }.meanOf("custom") { age / 2 } // [`city`, `custom`]    If statistic is applied in a mode that returns separate value per every column in data group, aggregated values will be stored in columns with original column names.    df.groupBy { city }.meanFor { age and weight } // [`city`, `age`, `weight`] df.groupBy { city }.mean() // [`city`, `age`, `weight`, ...]     When statistics is applied to Pivot or PivotGroupBy , it is computed for every data group.  If statistic is applied in a mode that returns a single value for every data group, it will be stored in matrix cell without any name.    df.groupBy { city }.pivot { name.lastName }.mean { age } df.groupBy { city }.pivot { name.lastName }.meanOf { age / 2 }    If statistic is applied in such a way that it returns separate value per every column in data group, every cell in matrix will contain DataRow with values for every aggregated column.    df.groupBy { city }.pivot { name.lastName }.meanFor { age and weight } df.groupBy { city }.pivot { name.lastName }.mean()    To group columns in aggregation results not by pivoted values, but by aggregated columns, apply separate flag:    df.groupBy { city }.pivot { name.lastName }.meanFor(separate = true) { age and weight } df.groupBy { city }.pivot { name.lastName }.mean(separate = true)
Counts the number of rows.    df.count()    Pass row condition to count number of rows that satisfy to that condition:    df.count { age > 15 }    When count is used in groupBy or pivot aggregations, it counts rows for every data group:    df.groupBy { city }.count() df.pivot { city }.count { age > 18 } df.pivot { name.firstName }.groupBy { name.lastName }.count()
Computes the minimum / maximum of values.  Is available for Comparable columns. null and NaN values are ignored.    df.min() // min of values per every comparable column df.min { age and weight } // min of all values in `age` and `weight` df.minFor { age and weight } // min of values per `age` and `weight` separately df.minOf { (weight ?: 0) / age } // min of expression evaluated for every row df.minBy { age } // DataRow with minimal `age`      df.min() df.age.min() df.groupBy { city }.min() df.pivot { city }.min() df.pivot { city }.groupBy { name.lastName }.min()    See statistics for details on complex data aggregations.
Computes the sum of values.  null and NaN values are ignored.    df.sum() // sum of values per every numeric column df.sum { age and weight } // sum of all values in `age` and `weight` df.sumFor { age and weight } // sum of values per `age` and `weight` separately df.sumOf { (weight ?: 0) / age } // sum of expression evaluated for every row      df.age.sum() df.groupBy { city }.sum() df.pivot { city }.sum() df.pivot { city }.groupBy { name.lastName }.sum()    See statistics for details on complex data aggregations.
Replaces one or several columns with new columns.  replace { columns } .with(newColumns) | .with { columnExpression } columnExpression: DataFrame.(DataColumn) -> DataColumn  See column selectors    df.replace { name }.with { name.firstName } df.replace { stringCols() }.with { it.lowercase() } df.replace { age }.with { 2021 - age named "year" }    Note: replace { columns }.with { columnExpression } is equivalent to convert { columns }.to { columnExpression } . See convert
explode - distributes lists of values or dataframes in given columns vertically, replicating data in other columns  implode - collects column values in given columns into lists or dataframes, grouping by other columns
Splits list-like values in one or several columns and spreads them vertically. Values in other columns are duplicated.  This is reverse operation to implode  Exploded columns will change their types:   List<T> to T  DataFrame to DataRow   Note that exploded FrameColumn will convert into ColumnGroup  Rows with empty lists will be skipped. If you want to keep such rows with null value in exploded columns, set dropEmpty flag to false .      val a by columnOf(1, 2) val b by columnOf(listOf(1, 2), listOf(3, 4)) val df = dataFrameOf(a, b) df.explode { b }    val df = dataFrameOf("a", "b")( 1, listOf(1, 2), 2, listOf(3, 4) ) df.explode("b")     When several columns are exploded in one operation, lists in different columns will be aligned.    val a by columnOf(listOf(1, 2), listOf(3, 4, 5)) val b by columnOf(listOf(1, 2, 3), listOf(4, 5)) val df = dataFrameOf(a, b) df.explode { a and b }    DataColumn<Collection> or FrameColumn can also be exploded:    val col by columnOf(listOf(1, 2), listOf(3, 4)) col.explode()      val col by columnOf( dataFrameOf("a", "b")(1, 2, 3, 4), dataFrameOf("a", "b")(5, 6, 7, 8) ) col.explode()
Returns DataFrame where values in given columns are merged into lists grouped by other columns.  This is reverse operation to explode  Imploded columns will change their types:   T to List<T>  DataRow to DataFrame   Note that imploded ColumnGroup will convert into FrameColumn    df.implode { name and age and weight and isHappy }    Set dropNulls flag to filter
groupBy - groups rows of DataFrame by given key columns  concat - concatenates rows from several DataFrames into single DataFrame
Splits the rows of DataFrame into groups using one or several columns as grouping keys.  groupBy { columns } [transformations] [aggregations]  See column selectors , groupBy transformations and groupBy aggregations      df.groupBy { name } df.groupBy { city and name.lastName } df.groupBy { age / 10 named "ageDecade" } df.groupBy { expr { name.firstName.length + name.lastName.length } named "nameLength" }    val name by columnGroup() val lastName by name.column<String>() val firstName by name.column<String>() val age by column<Int>() val city by column<String?>() df.groupBy { name } // or df.groupBy(name) df.groupBy { city and lastName } // or df.groupBy(city, lastName) df.groupBy { age / 10 named "ageDecade" } df.groupBy { expr { firstName().length + lastName().length } named "nameLength" }    df.groupBy("name") df.groupBy { "city" and "name"["lastName"] } df.groupBy { "age".ints() / 10 named "ageDecade" } df.groupBy { expr { "name"["firstName"]<String>().length + "name"["lastName"]<String>().length } named "nameLength" }     Returns GroupBy object.   GroupBy is a DataFrame with one chosen FrameColumn containing data groups.  It supports the following operations:   add  sortBy  map  pivot  concat   Any DataFrame with FrameColumn can be reinterpreted as GroupBy :    val key by columnOf(1, 2) // create int column with name "key" val data by columnOf(df[0..3], df[4..6]) // create frame column with name "data" val df = dataFrameOf(key, data) // create dataframe with two columns df.asGroupBy { data } // convert dataframe to GroupBy by interpreting 'data' column as groups    And any GroupBy can be reinterpreted as DataFrame with FrameColumn :    df.groupBy { city }.toDataFrame()    Use concat to union all data groups of GroupBy into original DataFrame preserving new order of rows produced by grouping:    df.groupBy { name }.concat()     To compute one or several statistics per every group of GroupBy use aggregate function. Its body will be executed for every data group and has a receiver of type DataFrame that represents current data group being aggregated. To add a new column to the resulting DataFrame , pass the name of new column to infix function into :      df.groupBy { city }.aggregate { nrow() into "total" count { age > 18 } into "adults" median { age } into "median age" min { age } into "min age" maxBy { age }.name into "oldest" }    val city by column<String?>() val age by column<Int>() val name by columnGroup() df.groupBy { city }.aggregate { nrow() into "total" count { age() > 18 } into "adults" median { age } into "median age" min { age } into "min age" maxBy { age() }[name] into "name of oldest" } // or df.groupBy(city).aggregate { nrow() into "total" count { age > 18 } into "adults" median(age) into "median age" min(age) into "min age" maxBy(age)[name] into "name of oldest" }    df.groupBy("city").aggregate { nrow() into "total" count { "age"<Int>() > 18 } into "adults" median("age") into "median age" min("age") into "min age" maxBy("age")["name"] into "oldest" }     If only one aggregation function is used, column name can be omitted:      df.groupBy { city }.aggregate { maxBy { age }.name }    val city by column<String?>() val age by column<Int>() val name by columnGroup() df.groupBy { city }.aggregate { maxBy { age() }[name] } // or df.groupBy(city).aggregate { maxBy(age)[name] }    df.groupBy("city").aggregate { maxBy("age")["name"] }     Most common aggregation functions can be computed directly at GroupBy :      df.groupBy { city }.max() // max for every comparable column df.groupBy { city }.mean() // mean for every numeric column df.groupBy { city }.max { age } // max age into column "age" df.groupBy { city }.sum("total weight") { weight } // sum of weights into column "total weight" df.groupBy { city }.count() // number of rows into column "count" df.groupBy { city } .max { name.firstName.length() and name.lastName.length() } // maximum length of firstName or lastName into column "max" df.groupBy { city } .medianFor { age and weight } // median age into column "age", median weight into column "weight" df.groupBy { city } .minFor { (age into "min age") and (weight into "min weight") } // min age into column "min age", min weight into column "min weight" df.groupBy { city }.meanOf("mean ratio") { weight?.div(age) } // mean of weight/age into column "mean ratio"    val city by column<String?>() val age by column<Int>() val weight by column<Int?>() val name by columnGroup() val firstName by name.column<String>() val lastName by name.column<String>() df.groupBy { city }.max() // max for every comparable column df.groupBy { city }.mean() // mean for every numeric column df.groupBy { city }.max { age } // max age into column "age" df.groupBy { city }.sum("total weight") { weight } // sum of weights into column "total weight" df.groupBy { city }.count() // number of rows into column "count" df.groupBy { city } .max { firstName.length() and lastName.length() } // maximum length of firstName or lastName into column "max" df.groupBy { city } .medianFor { age and weight } // median age into column "age", median weight into column "weight" df.groupBy { city } .minFor { (age into "min age") and (weight into "min weight") } // min age into column "min age", min weight into column "min weight" df.groupBy { city }.meanOf("mean ratio") { weight()?.div(age()) } // mean of weight/age into column "mean ratio"    df.groupBy("city").max() // max for every comparable column df.groupBy("city").mean() // mean for every numeric column df.groupBy("city").max("age") // max age into column "age" df.groupBy("city").sum("weight", name = "total weight") // sum of weights into column "total weight" df.groupBy("city").count() // number of rows into column "count" df.groupBy("city").max { "name"["firstName"].strings().length() and "name"["lastName"].strings().length() } // maximum length of firstName or lastName into column "max" df.groupBy("city") .medianFor("age", "weight") // median age into column "age", median weight into column "weight" df.groupBy("city") .minFor { ("age".ints() into "min age") and ("weight".intOrNulls() into "min weight") } // min age into column "min age", min weight into column "min weight" df.groupBy("city").meanOf("mean ratio") { "weight".intOrNull()?.div("age".int()) } // mean of weight/age into column "mean ratio"     To get all column values for every group without aggregation use values function:   for ValueColumn of type T it will gather group values into lists of type List<T>  for ColumnGroup it will gather group values into DataFrame and convert ColumnGroup into FrameColumn       df.groupBy { city }.values() df.groupBy { city }.values { name and age } df.groupBy { city }.values { weight into "weights" }    val city by column<String?>() val age by column<Int>() val weight by column<Int?>() val name by columnGroup() df.groupBy(city).values() df.groupBy(city).values(name, age) df.groupBy(city).values { weight into "weights" }    df.groupBy("city").values() df.groupBy("city").values("name", "age") df.groupBy("city").values { "weight" into "weights" }
Returns DataFrame with the union of rows from several given DataFrames .  concat is available for:  DataFrame :    df.concat(df1, df2)    Iterable<DataFrame> :    listOf(df1, df2).concat()    Iterable<DataRow> :    val rows = listOf(df[2], df[4], df[5]) rows.concat()    groupBy :    df.groupBy { name }.concat()    FrameColumn :    val frameColumn by columnOf(df[0..1], df[4..5]) frameColumn.concat()    If you want to union columns (not rows) from several DataFrames , see add .   If input DataFrames have different schemas, every column in resulting DataFrame will have the most common type of the original columns with the same name.  For example, if one DataFrame has column A: Int and other DataFrame has column A: Double , resulting DataFrame will have column A: Number .  Missing columns in dataframes will be filled with null .
Both update and convert can be used to change columns values in DataFrame .  Difference between these operations:   convert allows to change the type of the column, update doesn't  update allows to filter cells to be updated, convert doesn't
split column values horizontally or vertically  merge values from several columns into single column
Splits every value in the given columns into several values. Splitted values can be spread horizontally or vertically or remain inside the original column as List  The following types of columns can be splitted by default:   String : split by , and trim  List : split into elements  DataFrame : split into rows   df.split { columns } [.cast<Type>()] [.by(delimeters) | .by { splitter } | .match(regex)] // how to split cell value [.default(value)] // how to fill nulls .into(columnNames) [ { columnNamesGenerator } ] | .inward(columnNames) [ { columnNamesGenerator } | .inplace() | .intoRows() | .intoColumns() ] // where to store results splitter = DataRow.(T) -> Iterable<Any> columnNamesGenerator = DataColumn.(columnIndex: Int) -> String  Storage options:   into(col1, col2, ... ) - store splitted values in new top-level columns  inward(col1, col2, ...) - store splitted values in new columns nested inside original column  inplace - store splitted values in original column as List  intoRows - spread splitted values vertically into new rows  intoColumns - split FrameColumn into ColumnGroup storing in every cell a List of original values per every column   columnNamesGenerator is used to generate names for additional columns when the list of explicitly specified columnNames was not long enough. columnIndex starts with 1 for the first additional column name.  Default columnNamesGenerator generates column names splitted1 , splitted2 ...   Reverse operation to merge      df.split { name.firstName }.by { it.chars().toList() }.inplace() df.split { name }.by { it.values() }.into("nameParts") df.split { name.lastName }.by(" ").default("").inward { "word$it" }    val name by columnGroup() val firstName by name.column<String>() val lastName by name.column<String>() df.split { firstName }.by { it.chars().toList() }.inplace() df.split { name }.by { it.values() }.into("nameParts") df.split { lastName }.by(" ").default("").inward { "word$it" }    df.split { "name"["firstName"]<String>() }.by { it.chars().toList() }.inplace() df.split { name }.by { it.values() }.into("nameParts") df.split { "name"["lastName"] }.by(" ").default("").inward { "word$it" }     String columns can also be splitted into group matches of Regex pattern:    merged.split { name } .match("""(.*) \((.*)\)""") .inward("firstName", "lastName")    FrameColumn can be splitted into columns:    val df1 = dataFrameOf("a", "b", "c")( 1, 2, 3, 4, 5, 6 ) val df2 = dataFrameOf("a", "b")( 5, 6, 7, 8, 9, 10 ) val group by columnOf(df1, df2) val id by columnOf("x", "y") val df = dataFrameOf(id, group) df.split { group }.intoColumns()     Returns DataFrame with duplicated rows for every splitted value.  Reverse operation to implode .  Use .intoRows() terminal operation in split configuration to spread splitted values vertically:      df.split { name.firstName }.by { it.chars().toList() }.intoRows() df.split { name }.by { it.values() }.intoRows()    val name by columnGroup() val firstName by name.column<String>() df.split { firstName }.by { it.chars().toList() }.intoRows() df.split { name }.by { it.values() }.intoRows()    df.split { "name"["firstName"]<String>() }.by { it.chars().toList() }.intoRows() df.split { group("name") }.by { it.values() }.intoRows()     Equals to split { column }...inplace().explode { column } . See explode for details.
Merges several columns into a single column.  Reverse operation to split  merge { columns } [.notNull()] .by(delimeter) | .by { merger } [.into(column) | .intoList() ] merger: (DataRow).List<T> -> Any    // Merge two columns into one column "fullName" df.merge { name.firstName and name.lastName }.by(" ").into("fullName")    merger accepts a List of collected values for every row typed by their common type:    df.merge { name.firstName and name.lastName } .by { it[0] + " (" + it[1].uppercase() + ")" } .into("fullName")    When heterogeneous columns are merged, they may need to be cast to valid types in merger :    df.merge { name.firstName and age and isHappy } .by { "${it[0]} aged ${it[1]} is " + (if (it[2] as Boolean) "" else "not ") + "happy" } .into("status")    By default, when no delimeter or merger is specified, values will be merged into the List :    df.merge { numberCols() }.into("data")    Merged column values can also be exported to List :    // Merge data from two columns into List<String> df.merge { name.firstName and name.lastName }.by(",").intoList()
add columns to DataFrame  map columns to new DataFrame or DataColumn  remove columns from DataFrame
Return column by column name or column selector as DataColumn . Throws exception if requested column doesn't exist.      df.getColumn { age }    val age by column<Int>() df.getColumn { age }    df.getColumn("age")      Return top-level column by column name or column selector as DataColumn or null if requested column doesn't exist.      df.getColumnOrNull { age }    val age by column<Int>() df.getColumnOrNull(age)    df.getColumnOrNull("age")      Return top-level column by column name or column selector as ColumnGroup . Throws exception if requested column doesn't exist or is not a ColumnGroup .      df.getColumnGroup { name }    val name by columnGroup() df.getColumnGroup(name)    df.getColumnGroup("name")      Return list of selected columns.      df.getColumns { age and name }    val age by column<Int>() val name by columnGroup() df.getColumns { age and name }    df.getColumns("age", "name")
DataFrame object is immutable and all operations return a new instance of DataFrame .  Similar operations for columns or rows modification have different names:   Column operation Row operation  add append  remove drop  select filter  group groupBy  sortColumnsBy sortBy  join concat   Horizontal (column) modification:   add  flatten  group  insert  map  merge  move  remove  rename  replace  select  split  ungroup   Vertical (row) modification:   append  concat  drop  distinct  explode  filter  implode  shuffle  sortBy  split   Data modification:   convert  parse  update   Reshaping:   pivot  gather   Learn how to:   Slice rows  Filter rows  Reorder rows  Select columns  Update/convert values  Split/merge values  Group rows by keys  Append values  Add/map/remove columns  Move/rename columns  Insert/replace columns  Explode/implode columns  Pivot/gather columns
Returns DataFrame with rows at given indices:    df[0, 3, 4]    Returns DataFrame with rows inside given index ranges (including boundary indices):    df[1..2] df[0..2, 4..5]     Returns DataFrame containing first n rows    df.take(5)     Returns DataFrame containing last n rows    df.takeLast(5)     Returns DataFrame containing all rows except first n rows    df.drop(5)     Returns DataFrame containing all rows except last n rows    df.dropLast() // default 1 df.dropLast(5)
Return cross-section from the DataFrame .  Filters DataFrame by matching key values with key columns and removes key columns.  xs(vararg keyValues: Any?) [ { keyColumns } ]  When keyColumns are not specified, it takes first n columns in dfs order (looking inside ColumnGroups ), where n is a number of given keyValues .    df.xs("Charlie", "Chaplin") df.xs("Moscow", true) { city and isHappy }
In Jupyter environment DataFrame provides typed data access by automatic inference of DataSchema of new DataFrame instances and generation of schema-specific extension properties   After execution of cell    val df = dataFrameOf("name", "age")( "Alice", 15, "Bob", null )    the following actions take place:   Columns in df are analyzed to extract data schema  Empty interface with DataSchema annotation is generated:   @DataSchema interface DataFrameType   Extension properties for this DataSchema are generated:   val ColumnsContainer<DataFrameType>.age: DataColumn<Int?> @JvmName("DataFrameType_age") get() = this["age"] as DataColumn<Int?> val DataRow<DataFrameType>.age: Int? @JvmName("DataFrameType_age") get() = this["age"] as Int? val ColumnsContainer<DataFrameType>.name: DataColumn<String> @JvmName("DataFrameType_name") get() = this["name"] as DataColumn<String> val DataRow<DataFrameType>.name: String @JvmName("DataFrameType_name") get() = this["name"] as String  Every column produces two extension properties:   Property for ColumnsContainer<DataFrameType> returns column  Property for DataRow<DataFrameType> returns cell value    df variable is typed by schema interface:   val temp = df  val df = temp.cast<DataFrameType>()   Note, that object instance after typing remains the same   To log all these additional code executions, use cell magic  %trackExecution -all   In order to reduce amount of generated code, previously generated DataSchema interfaces are reused and only new properties are introduced  Let's filter out all null values from age column and add one more column of type Boolean :  val filtered = df.filter { age != null }.add("isAdult") { age!! > 18 }  New schema interface for filtered variable will be derived from previously generated DataFrameType :  @DataSchema interface DataFrameType1: DataFrameType  Extension properties for data access are generated only for new and overriden members of DataFrameType1 interface:  val ColumnsContainer<DataFrameType1>.age: DataColumn<Int> get() = this["age"] as DataColumn<Int> val DataRow<DataFrameType1>.age: Int get() = this["age"] as Int val ColumnsContainer<DataFrameType1>.isAdult: DataColumn<Boolean> get() = this["isAdult"] as DataColumn<Boolean> val DataRow<DataFrameType1>.isAdult: String get() = this["isAdult"] as Boolean  Then variable filtered is typed by new interface:  val temp = filtered  val filtered = temp as DataFrame<DataFrameType1>   Besides auto-generated schema interfaces, you can explicitly define your own data schema:  @DataSchema interface Person { val name: String val age: Int }  After execution of this cell in Jupyter, extension properties for data access will be generated. Now we can use these properties to create functions for typed DataFrame :  fun DataFrame<Person>.splitName() = split { name }.by(",").into("firstName", "lastName") fun DataFrame<Person>.adults() = filter { age > 18 }  These functions will work for any DataFrame that matches Person schema:    val df = dataFrameOf("name", "age", "weight")( "Merton, Alice", 15, 60.0, "Marley, Bob", 20, 73.5 )    Schema of df is compatible with Person , so auto-generated schema interface will inherit from it:  @DataSchema(isOpen = false) interface DataFrameType : Person val DataFrameBase<DataFrameType>.age: DataColumn<Double> get() = this["weight"] as DataColumn<Double> val DataRowBase<DataFrameType>.age: Double get() = this["weight"] as Int  Despite df has additional column weight , previously defined functions for DataFrame<Person> work for it:    df.splitName()    firstName lastName age weight 0 Merton Alice 15 60.000 1 Marley Bob 20 73.125    df.adults()    name age weight 0 Marley, Bob 20 73.5   Sometimes it is convenient to extract reusable code from Jupyter notebook into Kotlin JVM library. If this code uses Custom data schemas , schema interfaces should also be extracted. In order to enable support them in Jupyter, you should register them in library integration class with useSchema function:  @DataSchema interface Person { val name: String val age: Int } fun DataFrame<Person>.countAdults() = count { it[Person::age] > 18 } @JupyterLibrary internal class Integration : JupyterIntegration() { override fun Builder.onLoaded() { onLoaded { useSchema<Person>() } } }  After loading this library into Jupyter notebook, schema interfaces for all DataFrame variables that match Person schema will derive from Person    val df = dataFrameOf("name", "age")( "Alice", 15, "Bob", null )    Now df is assignable to DataFrame<Person> and countAdults is available:  df.countAdults()
In Gradle project DataFrame provides annotation processing and gradle tasks to infer DataSchema from data samples.   Declare data schemas in your code and use them to access data in DataFrames. A data schema is an interface with properties and no type parameters annotated with @DataSchema :  package org.example import org.jetbrains.kotlinx.dataframe.annotations.DataSchema @DataSchema interface Person { val name: String val age: Int }     val df = dataFrameOf("name", "age")( "Alice", 15, "Bob", 20 ).cast<Person>() // age only available after executing `build` or `kspKotlin`! val teens = df.filter { age in 10..19 } teens.print()     Specify schema's configurations in dataframes and execute the build task. For the following configuration, file Repository.Generated.kt will be generated. See reference and examples for more details.   dataframes { schema { data = "https://raw.githubusercontent.com/Kotlin/dataframe/master/data/jetbrains_repositories.csv" name = "org.example.Repository" } }  After build , the following code should compile and run:    val REPOSITORIES_DATA = "https://raw.githubusercontent.com/Kotlin/dataframe/master/data/jetbrains_repositories.csv" val df = DataFrame.read(REPOSITORIES_DATA).cast<Repository>() // Use generated properties to access data in rows df.maxBy { stargazers_count }.print() // Or to access columns in dataframe. print(df.full_name.count { it.contains("kotlin") })
You can use DataFrame in different environments - as any other JVM library. The following sections will show how to use DataFrame in Jupyter , Datalore and in a Gradle project .   You can use DataFrame in Jupyter Notebook and in Jupyter Lab. To start, install the latest version of Kotlin kernel and start your favorite Jupyter client from the command line, for example:  jupyter notebook  In the notebook you only have to write single line to start using dataframe:  %use dataframe  In this case the version which is bundled with the kernel, will be used. If you want to always use the latest version, add another magic before %use dataframe :  %useLatestDescriptors %use dataframe  If you want to use specific version of DataFrame, you can specify it in brackets:  %use dataframe(0.9)  After loading, all essential types will be already imported, so you can start using DataFrame. Enjoy!   To start with DataFrame in Datalore, create a Kotlin notebook first:    As the Notebook you've created is actually a Jupyter notebook, you can follow the instructions in the previous section to turn DataFrame on. The simplest way of doing this is shown on screenshot:     DataFrame is published to Maven Central, so you can simply add the following line to your Kotlin DSL buildscript to depend on it:  dependencies { implementation("org.jetbrains.kotlinx:dataframe:<version>") }  In Groovy DSL buildscript setup is very similar:  dependencies { implementation 'org.jetbrains.kotlinx:dataframe:<version>' }   We provide a Gradle plugin that generates interfaces by your data. To use it in your project, pick up the latest version from here and follow the configuration:    plugins { id("org.jetbrains.kotlin.plugin.dataframe") version "<version>" } dependencies { implementation("org.jetbrains.kotlinx:dataframe:<version>") } // Make IDE aware of the generated code: sourceSets { main.kotlin.srcDir("build/generated/ksp/main/kotlin/") } // (Only if you use kotlint) Excludes for `kotlint`: tasks.withType(org.jmailen.gradle.kotlinter.tasks.LintTask).all { exclude { it.name.endsWith(".Generated.kt") } exclude { it.name.endsWith("\$Extensions.kt") } }    plugins { id("org.jetbrains.kotlin.plugin.dataframe") version "<version>" } dependencies { implementation("org.jetbrains.kotlinx:dataframe:<version>") } // Make IDE aware of the generated code: kotlin.sourceSets.getByName("main").kotlin.srcDir("build/generated/ksp/main/kotlin/") // (Only if you use kotlint) Excludes for `kotlint`: tasks.withType<org.jmailen.gradle.kotlinter.tasks.LintTask> { exclude { it.name.endsWith(".Generated.kt") } exclude { it.name.endsWith("\$Extensions.kt") } }    plugins { id("org.jetbrains.kotlin.plugin.dataframe") version "<version>" } kotlin { jvm() sourceSets { val jvmMain by getting { // Make IDE aware of the generated code: kotlin.srcDir("build/generated/ksp/jvmMain/kotlin/") dependencies { implementation("org.jetbrains.kotlinx:dataframe:<version>") } } } } // (Only if you use kotlint) Excludes for `kotlint`: tasks.withType<org.jmailen.gradle.kotlinter.tasks.LintTask> { exclude { it.name.endsWith(".Generated.kt") } exclude { it.name.endsWith("\$Extensions.kt") } }    Note that it's better to use the same version for a library and plugin to avoid unpredictable errors.   If you are using Maven, Ivy or Bazel to configure your build, you can still use DataFrame in your project. Just follow the instructions for your build system on this page .
DataColumn is a named, typed and ordered collection of elements  DataFrame consists of one or several DataColumns with unique names and equal size  DataRow is a single row of DataFrame and provides a single value for every DataColumn
Replaces ColumnGroup with its nested columns.  Reverse operation to group  ungroup { columns }  See column selectors    // name.firstName -> firstName // name.lastName -> lastName df.ungroup { name }
Returns DataFrame without column groupings under selected columns  flatten [ { columns } ]    // name.firstName -> firstName // name.lastName -> lastName df.flatten { name }    Potential column name clashes are resolved by adding minimal required prefix from ancestor column names.  To remove all column groupings in DataFrame , invoke flatten without parameters:    df.flatten()
insert - inserts new column into DataFrame  replace - replaces columns in DataFrame
Inserts new column at specific position in DataFrame .  Similar to add , but supports column positioning.  insert (columnName) { rowExpression } | (column) .under { parentColumn } | .after { column } | .at(position) rowExpression: DataRow.(DataRow) -> Value  Create new column based on existing columns and insert it into DataFrame :      df.insert("year of birth") { 2021 - age }.after { age }    val year = column<Int>("year of birth") val age by column<Int>() df.insert(year) { 2021 - age }.after { age }    df.insert("year of birth") { 2021 - "age"<Int>() }.after("age")     Insert previously created column:    val score by columnOf(4, 5, 3, 5, 4, 5, 3) df.insert(score).at(2)
Moves one or several columns within DataFrame .  move { columns } .into { pathSelector } | .under { parentColumn } | .after { column } | .to(position) | .toTop() | .toLeft() | .toRight() pathSelector: DataFrame.(DataColumn) -> ColumnPath  See Column Selectors  Can be used to change columns hierarchy by providing ColumnPath for every moved column    df.move { age }.toLeft() df.move { weight }.to(1) // age -> info.age // weight -> info.weight df.move { age and weight }.into { pathOf("info", it.name) } df.move { age and weight }.into { "info"[it.name] } df.move { age and weight }.under("info") // name.firstName -> fullName.first // name.lastName -> fullName.last df.move { name.firstName and name.lastName }.into { pathOf("fullName", it.name.dropLast(4)) } // a|b|c -> a.b.c // a|d|e -> a.d.e dataFrameOf("a|b|c", "a|d|e")(0, 0) .move { all() }.into { it.name.split("|").toPath() } // name.firstName -> firstName // name.lastName -> lastName df.move { name.cols() }.toTop() // a.b.e -> be // c.d.e -> de df.move { dfs { it.name == "e" } }.toTop { it.parent!!.name + it.name }    Special cases of move :   group - groups columns into ColumnGroups  ungroup - ungroups ColumnGroups  flatten - removes all column groupings
Renames one or several columns without changing its location in DataFrame  df.rename { columns }.into(name) df.rename { columns }.into { nameExpression } nameExpression = (DataColumn) -> String
group - groups given columns into ColumnGroups .  ungroup - ungroups given ColumnGroups by replacing them with their children columns  flatten - recursively removes all column groupings under given ColumnGroups , remaining only ValueColumns and FrameColumns   These operations are special cases of general move operation.
Group columns into ColumnsGroups .  It is a special case of move operation.  group { columns } .into(groupName) | .into { groupNameExpression } groupNameExpression = DataColumn.(DataColumn) -> String    df.group { age and city }.into("info") df.group { all() }.into { it.type().toString() }.print()    To ungroup grouped columns use ungroup operation.